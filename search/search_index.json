{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Generic Gen3 Data Commons Documentation \uf0c1 Overview The Center for Data Intensive Science (CDIS) at the University of Chicago has developed and maintains the Gen3 software stack to help accelerate scientific discovery through creation of a collaborative infrastructure that enables sharing of information between stakeholders in industry, academia, and regulatory agencies. The Gen3 software stack is a collection of microservices that enable the standing-up of data commons, which allows different partner organizations to pool data and grants approved researchers access to harmonized datasets in a scalable, reproducible, and secure manner. This documentation describes the commons and provides a user guide to assist data contributors and analysts . Guiding Principles OPEN DATA We believe that data must be open and accessible within the research community to collectively achieve the critical mass of data necessary to power data-driven research, insight, and discovery. OPEN-SOURCE We believe that collaboration creates a knowledge pool that not only drives better software development, but also connects us to a active community in pursuit of shared social impact. We have long benefitted from open-source software and are committed to contributing to future generations of software and scholars. OPEN INFRASTRUCTURE We believe that rapid innovation is most effectively achieved through an open infrastructure environment where portability and compatibility are maximized and knowledge is distributed broadly. For more information visit: CDIS Guiding Principles Support Operation of the Gen3 commons is supported by generous grants from Amazon Web Services' Grants for Research Credit Program and Microsoft Azure's Research Grant Program . The Data Commons Architecture \uf0c1 User access to the Gen3 data commons runs through a VPC or Virtual Private Cloud. Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security. All access is through a monitored head node. Data is not directly accessed from the Internet. Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API. Diagram of the System Architecture Contact CDIS Staff \uf0c1 Locations: Shoreland : 5454 South Shore Drive Suite 2B Chicago, IL 60615 University of Chicago : 900 East 57th Street 10th Floor, Room 10148 Chicago, IL 60616 Email: General Inquiries: Technical Support: and","title":"Home"},{"location":"#welcome-to-the-generic-gen3-data-commons-documentation","text":"","title":"Welcome to the Generic Gen3 Data Commons Documentation"},{"location":"#the-data-commons-architecture","text":"User access to the Gen3 data commons runs through a VPC or Virtual Private Cloud. Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security. All access is through a monitored head node. Data is not directly accessed from the Internet. Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.","title":"The Data Commons Architecture"},{"location":"#contact-cdis-staff","text":"Locations: Shoreland : 5454 South Shore Drive Suite 2B Chicago, IL 60615 University of Chicago : 900 East 57th Street 10th Floor, Room 10148 Chicago, IL 60616 Email: General Inquiries: Technical Support: and","title":"Contact CDIS Staff"},{"location":"about/contact/","text":"Contact the Gen3 Commons team \uf0c1 Address: Telephone: Email: Fax:","title":"Contact the Gen3 Commons team"},{"location":"about/contact/#contact-the-gen3-commons-team","text":"Address: Telephone: Email: Fax:","title":"Contact the Gen3 Commons team"},{"location":"about/gen3-overview/","text":"![CDIS Logo]({{ \"/assets/cdis.png\" | absolute_url }}) Overview \uf0c1 The Gen3 commons will allow approved researchers access to raw unprocessed datasets in a scalable, reproducible, privacy and security protected manner. Mission \uf0c1 The Gen3 data commons were developed to accelerate scientific discovery through creation of a collaborative infrastructure that enables sharing of information between stakeholders in industry, academia, and regulatory agencies. Gen3 is managed by the Center for Data Intensive Science (CDIS) at the University of Chicago. For more information visit: CDIS Guiding Principles Support \uf0c1 Operation of the Gen3 commons is supported by generous grants from Amazon Web Services' Grants for Research Credit Program and Microsoft Azure's Research Grant Program .","title":"Gen3 overview"},{"location":"about/gen3-overview/#overview","text":"The Gen3 commons will allow approved researchers access to raw unprocessed datasets in a scalable, reproducible, privacy and security protected manner.","title":"Overview"},{"location":"about/gen3-overview/#mission","text":"The Gen3 data commons were developed to accelerate scientific discovery through creation of a collaborative infrastructure that enables sharing of information between stakeholders in industry, academia, and regulatory agencies. Gen3 is managed by the Center for Data Intensive Science (CDIS) at the University of Chicago. For more information visit: CDIS Guiding Principles","title":"Mission"},{"location":"about/gen3-overview/#support","text":"Operation of the Gen3 commons is supported by generous grants from Amazon Web Services' Grants for Research Credit Program and Microsoft Azure's Research Grant Program .","title":"Support"},{"location":"appendices/api-gen3/","text":"Working with the API \uf0c1 What does the API do? \uf0c1 The API is created programmatically based on the Gen3 commons data model . All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps 4-6 in the Data Contribution section ). With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons. The API can be queried programmatically or through provided tools like the submission portal. We use GraphQL to manage the metadata in the Gen3 commons. To learn the basics of writing queries in Graph QL, we recommend this introduction . What's an example of the API at work? \uf0c1 The Gen3 commons team has created a few matrices to help describe submitted data. These are linked to from: https://www.gen3.org/data-group/ These query the API for the desired metadata and return the matrices. They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear. If you are a member and would like to view these matrices, contact info@gen3.org for a username and password. Credentials to query the API \uf0c1 Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you programmatically query or submit data to the API. This credential is used every time you make an API call. Each API key is valid for a month and is used to receive a temporary access token that is valid for only 30 minutes. The access token is what must be sent to the Gen3 API to access data in the commons. For users granted data access, the API key is provided on your Profile page when you click \"Create API key\". While displayed, you can click \"copy\" to copy the API key to your clipboard or \"download\" to download a \"credentials.json\" file containing the id/key pair in json format. In python, the following command is sent, using the module \"requests\", to receive the access token: # Save your copied credentials.json from the website into a variable key : key = { api_key : actual-key , key_id : a-key-uuid } # Import the requests python module: import requests # Pass the API key to the Gen3 API using requests.post to receive the access token: token = requests.post('https://gen3.commons.io/user/credentials/cdis/access_token', json=key).json() # Now you should see your access_token displayed when you enter: token When submitting a graphQL query to the Gen3 API, or requesting data download/upload, include the access token in your request header: headers = {'Authorization': 'bearer '+ token['access_token']} # a graphQL endpoint query using the key json: query = {'query': {project(first:0){project_id id}} }; ql = requests.post('https://gen3.commons.io/api/v0/submission/graphql/', json=query, headers=headers) print(ql.text) # display the response # Data download via API endpoint request: durl = 'https://gen3.commons.io/api/v0/submission/ program / project /export?format=tsv ids=' + ids[0:-1] # define the download url with the UUIDs of the records to download in ids list dl = requests.get(durl, headers=headers) print(dl.text) # display response # Data upload via API endpoint request: headers['content-type']='text/tab-separated-values' # add the content-type to header u = requests.put('https://gen3.commons.io/api/v0/submission/project-id', data=tsv, headers=headers) If you receive an error like \"You don't have access... \", then you will most likely need to update your API key or request a new access token. Queries in the submission portal: graphQL \uf0c1 You can run queries directly in the submission portal by clicking the \"Query\" magnifying glass or directly at: https://data.gen3.org/graphql . Queries are essential as you begin analyses. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need. Pagination and Offsets: Default = first 10 entries \uf0c1 Queries by defult return the first 10 entries. If you want more than that, you can specify it in the query call: (first:1000) In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with (first:3000) , then break your query into multiple queries with offsets: (first:1000, offset:0) # this will return records 0-1000 (first:1000, offset:1000) # this will return records 1000-2000 (first:1000, offset:2000) # this will return records 2000-2,550 Updating the example template details from experiment sample query to call the first 1000, the call becomes: { query : query Test { experiment (first:1000, submitter_id: INSERT submitter_id ) { experimental_intent experimental_description number_samples_per_experimental_group type_of_sample type_of_specimen } } } Browsing by project node \uf0c1 The metadata submission portal https://data.gen3.org/ can be used to browse an individual submission by node. Just select a project and then click the \"browse nodes\" button to the right. From there, you'll get a screen like below where you can query by node in the dropdown at the left. Example: Browse by node You can also use this feature to download the tsv associated with the node, or if you have \"write\" access to the this project, delete existing nodes. Graphing a project \uf0c1 You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph. Example: Graphing a project","title":"Working with the API"},{"location":"appendices/api-gen3/#working-with-the-api","text":"","title":"Working with the API"},{"location":"appendices/api-gen3/#what-does-the-api-do","text":"The API is created programmatically based on the Gen3 commons data model . All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps 4-6 in the Data Contribution section ). With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons. The API can be queried programmatically or through provided tools like the submission portal. We use GraphQL to manage the metadata in the Gen3 commons. To learn the basics of writing queries in Graph QL, we recommend this introduction .","title":"What does the API do?"},{"location":"appendices/api-gen3/#whats-an-example-of-the-api-at-work","text":"The Gen3 commons team has created a few matrices to help describe submitted data. These are linked to from: https://www.gen3.org/data-group/ These query the API for the desired metadata and return the matrices. They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear. If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.","title":"What's an example of the API at work?"},{"location":"appendices/api-gen3/#credentials-to-query-the-api","text":"Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you programmatically query or submit data to the API. This credential is used every time you make an API call. Each API key is valid for a month and is used to receive a temporary access token that is valid for only 30 minutes. The access token is what must be sent to the Gen3 API to access data in the commons. For users granted data access, the API key is provided on your Profile page when you click \"Create API key\". While displayed, you can click \"copy\" to copy the API key to your clipboard or \"download\" to download a \"credentials.json\" file containing the id/key pair in json format. In python, the following command is sent, using the module \"requests\", to receive the access token: # Save your copied credentials.json from the website into a variable key : key = { api_key : actual-key , key_id : a-key-uuid } # Import the requests python module: import requests # Pass the API key to the Gen3 API using requests.post to receive the access token: token = requests.post('https://gen3.commons.io/user/credentials/cdis/access_token', json=key).json() # Now you should see your access_token displayed when you enter: token When submitting a graphQL query to the Gen3 API, or requesting data download/upload, include the access token in your request header: headers = {'Authorization': 'bearer '+ token['access_token']} # a graphQL endpoint query using the key json: query = {'query': {project(first:0){project_id id}} }; ql = requests.post('https://gen3.commons.io/api/v0/submission/graphql/', json=query, headers=headers) print(ql.text) # display the response # Data download via API endpoint request: durl = 'https://gen3.commons.io/api/v0/submission/ program / project /export?format=tsv ids=' + ids[0:-1] # define the download url with the UUIDs of the records to download in ids list dl = requests.get(durl, headers=headers) print(dl.text) # display response # Data upload via API endpoint request: headers['content-type']='text/tab-separated-values' # add the content-type to header u = requests.put('https://gen3.commons.io/api/v0/submission/project-id', data=tsv, headers=headers) If you receive an error like \"You don't have access... \", then you will most likely need to update your API key or request a new access token.","title":"Credentials to query the API"},{"location":"appendices/api-gen3/#queries-in-the-submission-portal-graphql","text":"You can run queries directly in the submission portal by clicking the \"Query\" magnifying glass or directly at: https://data.gen3.org/graphql . Queries are essential as you begin analyses. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need.","title":"Queries in the submission portal:   graphQL"},{"location":"appendices/api-gen3/#pagination-and-offsets-default-first-10-entries","text":"Queries by defult return the first 10 entries. If you want more than that, you can specify it in the query call: (first:1000) In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with (first:3000) , then break your query into multiple queries with offsets: (first:1000, offset:0) # this will return records 0-1000 (first:1000, offset:1000) # this will return records 1000-2000 (first:1000, offset:2000) # this will return records 2000-2,550 Updating the example template details from experiment sample query to call the first 1000, the call becomes: { query : query Test { experiment (first:1000, submitter_id: INSERT submitter_id ) { experimental_intent experimental_description number_samples_per_experimental_group type_of_sample type_of_specimen } } }","title":"Pagination and Offsets:   Default = first 10 entries"},{"location":"appendices/api-gen3/#browsing-by-project-node","text":"The metadata submission portal https://data.gen3.org/ can be used to browse an individual submission by node. Just select a project and then click the \"browse nodes\" button to the right. From there, you'll get a screen like below where you can query by node in the dropdown at the left.","title":"Browsing by project node"},{"location":"appendices/api-gen3/#graphing-a-project","text":"You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.","title":"Graphing a project"},{"location":"appendices/api/","text":"Working with the API \uf0c1 What does the API do? \uf0c1 The API is created programmatically based on the Gen3 commons data model . All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps 4-6 in the Data Contribution section ). With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons. The API can be queried programmatically or through provided tools like the submission portal. We use GraphQL to manage the metadata in the Gen3 commons. To learn the basics of writing queries in Graph QL, we recommend this introduction . What's an example of the API at work? \uf0c1 The Gen3 commons team has created a few matrices to help describe submitted data. These are linked to from: https://www.gen3.org/data-group/ These query the API for the desired metadata and return the matrices. They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear. If you are a member and would like to view these matrices, contact info@gen3.org for a username and password. SECRETS! Credentials to query \uf0c1 Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you query the API. For users granted data access, these are in a .secrets file in the home directory of your VM. These are used for every API call. For example: If you're doing the Jupyter notebook demo from your analysis VM, your secrets file is loaded early in the demo so you can query. If you're using the submission portal to query, your secrets file is setup in a DB associated with your login credentials. If you receive an error like \"You don't have access to this data\", then you will most likely need to update your API keys and enter them into your VM's .secrets file. 1) Click \"Create access key\" button: 2) Copy the displayed keys: 3) In your VM, open your .secrets file with a text editor: 4) Paste your new keys into the .secrets file and save it: Queries in the submission portal \uf0c1 You can run queries directly in the submission portal by clicking the magnifying glass or directly at: https://data.gen3.org/graphql . Queries are essential as you begin analysis. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need. Example templates have been setup here . Extra: Default = first 10 entries \uf0c1 Queries by defult return the first 10 entries. If you want more than that, you can specify it in the query call: (first:1000) In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with (first:3000) , then break your query into multiple queries with offsets: (first:1000, offset:0) # this will return records 0-1000 (first:1000, offset:1000) # this will return records 1000-2000 (first:1000, offset:2000) # this will return records 2000-2,550 Updating the example template details from experiment sample query to call the first 1000, the call becomes: { query : query Test { experiment (first:1000, submitter_id: INSERT submitter_id ) { experimental_intent experimental_description number_samples_per_experimental_group type_of_sample type_of_specimen } } } Browsing by project node \uf0c1 The metadata submission portal https://data.gen3.org/ can be used to browse an individual submission by node. Just select a project and then click the \"browse nodes\" button to the right. From there, you'll get a screen like below where you can query by node in the dropdown at the left. Example: Browse by node You can also use this feature to download the tsv associated with the node, or if you have \"write\" access to the this project, delete existing nodes. Graphing a project \uf0c1 You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph. Example: Graphing a project","title":"Working with the API"},{"location":"appendices/api/#working-with-the-api","text":"","title":"Working with the API"},{"location":"appendices/api/#what-does-the-api-do","text":"The API is created programmatically based on the Gen3 commons data model . All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps 4-6 in the Data Contribution section ). With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons. The API can be queried programmatically or through provided tools like the submission portal. We use GraphQL to manage the metadata in the Gen3 commons. To learn the basics of writing queries in Graph QL, we recommend this introduction .","title":"What does the API do?"},{"location":"appendices/api/#whats-an-example-of-the-api-at-work","text":"The Gen3 commons team has created a few matrices to help describe submitted data. These are linked to from: https://www.gen3.org/data-group/ These query the API for the desired metadata and return the matrices. They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear. If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.","title":"What's an example of the API at work?"},{"location":"appendices/api/#secrets-credentials-to-query","text":"Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you query the API. For users granted data access, these are in a .secrets file in the home directory of your VM. These are used for every API call. For example: If you're doing the Jupyter notebook demo from your analysis VM, your secrets file is loaded early in the demo so you can query. If you're using the submission portal to query, your secrets file is setup in a DB associated with your login credentials. If you receive an error like \"You don't have access to this data\", then you will most likely need to update your API keys and enter them into your VM's .secrets file. 1) Click \"Create access key\" button: 2) Copy the displayed keys: 3) In your VM, open your .secrets file with a text editor: 4) Paste your new keys into the .secrets file and save it:","title":"SECRETS!   Credentials to query"},{"location":"appendices/api/#queries-in-the-submission-portal","text":"You can run queries directly in the submission portal by clicking the magnifying glass or directly at: https://data.gen3.org/graphql . Queries are essential as you begin analysis. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need. Example templates have been setup here .","title":"Queries in the submission portal"},{"location":"appendices/api/#extra-default-first-10-entries","text":"Queries by defult return the first 10 entries. If you want more than that, you can specify it in the query call: (first:1000) In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with (first:3000) , then break your query into multiple queries with offsets: (first:1000, offset:0) # this will return records 0-1000 (first:1000, offset:1000) # this will return records 1000-2000 (first:1000, offset:2000) # this will return records 2000-2,550 Updating the example template details from experiment sample query to call the first 1000, the call becomes: { query : query Test { experiment (first:1000, submitter_id: INSERT submitter_id ) { experimental_intent experimental_description number_samples_per_experimental_group type_of_sample type_of_specimen } } }","title":"Extra:   Default = first 10 entries"},{"location":"appendices/api/#browsing-by-project-node","text":"The metadata submission portal https://data.gen3.org/ can be used to browse an individual submission by node. Just select a project and then click the \"browse nodes\" button to the right. From there, you'll get a screen like below where you can query by node in the dropdown at the left.","title":"Browsing by project node"},{"location":"appendices/api/#graphing-a-project","text":"You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.","title":"Graphing a project"},{"location":"appendices/architecture/","text":"Gen3 Data Commons Architecture \uf0c1 User access to the Gen3 Commons runs through a VPC or Virtual Private Cloud. Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security. All access is through a monitored head node. Data is not directly accessed from the Internet. Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API. Diagram of the Gen3 System Architecture","title":"Data Commons Architecture"},{"location":"appendices/architecture/#gen3-data-commons-architecture","text":"User access to the Gen3 Commons runs through a VPC or Virtual Private Cloud. Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security. All access is through a monitored head node. Data is not directly accessed from the Internet. Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.","title":"Gen3 Data Commons Architecture"},{"location":"appendices/cdis-data-client/","text":"Download and Upload Files using the CDIS Data Client \uf0c1 The CDIS data client provides an easy-to-use, command-line interface for uploading and downloading data files from a Gen3 data commons. 1) Installation Instructions \uf0c1 The CDIS data client can be downloaded as a compiled binary file for Windows, Linux or Mac OS, or it can be installed from source using Google's GO language . 1) To install the binary (recommended), simply download the binary file to the location of your choice and add that location to your path. The program is executed from the command-line by running the command /path/to/binary/cdis-data-client options 2) To install using GO: 1) Install Go tools: https://golang.org/doc/install 2) Download and install cdis-data-client: mkdir -p $GOPATH/src/github.com/uc-cdis` sudo mkdir -p /usr/local/go/src/github.com/uc-cdis cd $GOPATH/src/github.com/uc-cdis` cd /usr/local/go/src/github.com/uc-cdis git clone git@github.com:uc-cdis/cdis-data-client.git cd cdis-data-client go get -d ./... go install . The program can be executed from the command-line after adding /path/to/go/bin to your $PATH environment variable, e.g.: export PATH=$PATH:/usr/local/go/bin 2) Configure a Profile with Credentials \uf0c1 The CDIS data client needs to be configured with API credentials downloaded from the user's data commons profile, usually downloaded as a file named 'credentials.json'. After running the configure command, the user will be prompted to enter the API endpoint of the commons. Example: ./cdis-data-client configure --profile profile-name --cred /path/to/credentials.json ./cdis-data-client configure --profile bc --cred /Users/Me/Documents/creds/bc-credentials.json terminal-prompt $ API endpoint: https://data.braincommons.org/ When successfully executed, this will create a configuration file located at, e.g., ~/.cdis/config , which contains all the API keys and urls associated with each commons profile configured. 3) Upload a data file using its UUID \uf0c1 When metadata records are created in any node in a Gen3 data portal, these records are assigned a unique, 128-bit ID called a 'UUID'. When metadata records describing a data file in object storage are created in a project, the URL of the data file's address in object storage is specified to \"register\" the file . Once a data file is registered by creating its metadata record in the data portal, its UUID can be provided to the CDIS data client along with the local path of the file to upload that file to object storage. Example: ./cdis-data-client upload --profile profile-name --uuid f6923cf3-xxxx-xxxx-xxxx-14ab3f84f9d6 --file= ~/Documents/file_to_upload ./cdis-data-client upload --profile bc --uuid b82ff17c-2fc0-475a-9c21-50c19950b082 --file= test.txt 4) Download a data file using its UUID \uf0c1 Once a data file is registered and uploaded to object storage, its UUID can be provided to the CDIS data client to download the file. A filename can also be specified for the local copy of the file with the --file filename option. Example: ./cdis-data-client download --profile profile-name --uuid 206dfaa6-bcf1-4bc9-b2d0-77179f0f48fc --file= ~/Documents/name_the_file.json ./cdis-data-client download --profile bc --uuid b82ff17c-2fc0-475a-9c21-50c19950b082 --file= test.txt 5) Provide a manifest file for bulk upload/download \uf0c1 The CDIS data client currently does not take a manifest file; this functionality is currently being implemented. In the meantime, users who need to semi-automate upload or download of a collection of files can modify the following shell script to fit their needs. The following shell script can be run from the command-line like: sh cdc_manifest.sh profile_name manifest_filename Where profile_name is the name of the profile configured in step 2 above, and manifest_filename is the filename/path of the manifest file. The manifest file should not contain headers and each row should be only a UUID and filename separated by a tab, for example: a82ff17c-2fc0-475a-9c21-50c19950b082 filename-1.txt b82ff17c-2fc0-475a-9c21-50c19950b082 filename-2.txt c82ff17c-2fc0-475a-9c21-50c19950b082 filename-3.txt Here is the example shell script for uploading files. The script text should be added to a file named, e.g., 'cdc_manifest.sh', and the script should then be run in the directory containing the files: #!/bin/bash cat $2 | while read line do f=`echo $line | tr -d '\\r'` uuid=$(echo $f | awk '{print $1}') fname=$(echo $f | awk '{print $2}') if [ -f $fname ] then echo Uploading file $fname with UUID $uuid cdis-data-client upload --profile $1 --uuid $uuid --file $fname echo Upload of $fname complete! else echo $file not found. exit 1 fi done","title":"CDIS Data Client"},{"location":"appendices/cdis-data-client/#download-and-upload-files-using-the-cdis-data-client","text":"The CDIS data client provides an easy-to-use, command-line interface for uploading and downloading data files from a Gen3 data commons.","title":"Download and Upload Files using the CDIS Data Client"},{"location":"appendices/cdis-data-client/#1-installation-instructions","text":"The CDIS data client can be downloaded as a compiled binary file for Windows, Linux or Mac OS, or it can be installed from source using Google's GO language . 1) To install the binary (recommended), simply download the binary file to the location of your choice and add that location to your path. The program is executed from the command-line by running the command /path/to/binary/cdis-data-client options 2) To install using GO: 1) Install Go tools: https://golang.org/doc/install 2) Download and install cdis-data-client: mkdir -p $GOPATH/src/github.com/uc-cdis` sudo mkdir -p /usr/local/go/src/github.com/uc-cdis cd $GOPATH/src/github.com/uc-cdis` cd /usr/local/go/src/github.com/uc-cdis git clone git@github.com:uc-cdis/cdis-data-client.git cd cdis-data-client go get -d ./... go install . The program can be executed from the command-line after adding /path/to/go/bin to your $PATH environment variable, e.g.: export PATH=$PATH:/usr/local/go/bin","title":"1) Installation Instructions"},{"location":"appendices/cdis-data-client/#2-configure-a-profile-with-credentials","text":"The CDIS data client needs to be configured with API credentials downloaded from the user's data commons profile, usually downloaded as a file named 'credentials.json'. After running the configure command, the user will be prompted to enter the API endpoint of the commons. Example: ./cdis-data-client configure --profile profile-name --cred /path/to/credentials.json ./cdis-data-client configure --profile bc --cred /Users/Me/Documents/creds/bc-credentials.json terminal-prompt $ API endpoint: https://data.braincommons.org/ When successfully executed, this will create a configuration file located at, e.g., ~/.cdis/config , which contains all the API keys and urls associated with each commons profile configured.","title":"2) Configure a Profile with Credentials"},{"location":"appendices/cdis-data-client/#3-upload-a-data-file-using-its-uuid","text":"When metadata records are created in any node in a Gen3 data portal, these records are assigned a unique, 128-bit ID called a 'UUID'. When metadata records describing a data file in object storage are created in a project, the URL of the data file's address in object storage is specified to \"register\" the file . Once a data file is registered by creating its metadata record in the data portal, its UUID can be provided to the CDIS data client along with the local path of the file to upload that file to object storage. Example: ./cdis-data-client upload --profile profile-name --uuid f6923cf3-xxxx-xxxx-xxxx-14ab3f84f9d6 --file= ~/Documents/file_to_upload ./cdis-data-client upload --profile bc --uuid b82ff17c-2fc0-475a-9c21-50c19950b082 --file= test.txt","title":"3) Upload a data file using its UUID"},{"location":"appendices/cdis-data-client/#4-download-a-data-file-using-its-uuid","text":"Once a data file is registered and uploaded to object storage, its UUID can be provided to the CDIS data client to download the file. A filename can also be specified for the local copy of the file with the --file filename option. Example: ./cdis-data-client download --profile profile-name --uuid 206dfaa6-bcf1-4bc9-b2d0-77179f0f48fc --file= ~/Documents/name_the_file.json ./cdis-data-client download --profile bc --uuid b82ff17c-2fc0-475a-9c21-50c19950b082 --file= test.txt","title":"4) Download a data file using its UUID"},{"location":"appendices/cdis-data-client/#5-provide-a-manifest-file-for-bulk-uploaddownload","text":"The CDIS data client currently does not take a manifest file; this functionality is currently being implemented. In the meantime, users who need to semi-automate upload or download of a collection of files can modify the following shell script to fit their needs. The following shell script can be run from the command-line like: sh cdc_manifest.sh profile_name manifest_filename Where profile_name is the name of the profile configured in step 2 above, and manifest_filename is the filename/path of the manifest file. The manifest file should not contain headers and each row should be only a UUID and filename separated by a tab, for example: a82ff17c-2fc0-475a-9c21-50c19950b082 filename-1.txt b82ff17c-2fc0-475a-9c21-50c19950b082 filename-2.txt c82ff17c-2fc0-475a-9c21-50c19950b082 filename-3.txt Here is the example shell script for uploading files. The script text should be added to a file named, e.g., 'cdc_manifest.sh', and the script should then be run in the directory containing the files: #!/bin/bash cat $2 | while read line do f=`echo $line | tr -d '\\r'` uuid=$(echo $f | awk '{print $1}') fname=$(echo $f | awk '{print $2}') if [ -f $fname ] then echo Uploading file $fname with UUID $uuid cdis-data-client upload --profile $1 --uuid $uuid --file $fname echo Upload of $fname complete! else echo $file not found. exit 1 fi done","title":"5) Provide a manifest file for bulk upload/download"},{"location":"appendices/data-dictionary/","text":"Data Dictionary Viewer \uf0c1 The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node. This is an invaluable tool for both the submission of data and later analysis of the entire commons. In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables. Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: https://data.Gen3.org/dd/ Viewing data dictionary as a graph: \uf0c1 Toggling the graph view: \uf0c1 Viewing data dictionary as tables: \uf0c1 Toggling between different views \uf0c1","title":"Data Dictionary"},{"location":"appendices/data-dictionary/#data-dictionary-viewer","text":"The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node. This is an invaluable tool for both the submission of data and later analysis of the entire commons. In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables. Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: https://data.Gen3.org/dd/","title":"Data Dictionary Viewer"},{"location":"appendices/data-dictionary/#viewing-data-dictionary-as-a-graph","text":"","title":"Viewing data dictionary as a graph:"},{"location":"appendices/data-dictionary/#toggling-the-graph-view","text":"","title":"Toggling the graph view:"},{"location":"appendices/data-dictionary/#viewing-data-dictionary-as-tables","text":"","title":"Viewing data dictionary as tables:"},{"location":"appendices/data-dictionary/#toggling-between-different-views","text":"","title":"Toggling between different views"},{"location":"appendices/mtde/","text":"Minimum Technical Data Elements (MTDE) \uf0c1 To facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the \u201cMinimum Technical Data Elements\u201d. These pre-analytic fields were determined through an iterative series of conversations with data contributors and the Gen3 \u201cData Experience\u201d group. For example, if a data commons was designed to accept gene/genome sequences, in order to facilitate analysis of genetic data across studies submitted by different groups, a node called \"submitted aligned reads\" may require the following metadata properties (MTDE) labeled as \"Required: Yes\": Submitted Aligned Reads Download template: JSON | TSV","title":"Minimum Technical Data Elements (MTDE)"},{"location":"appendices/mtde/#minimum-technical-data-elements-mtde","text":"To facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the \u201cMinimum Technical Data Elements\u201d. These pre-analytic fields were determined through an iterative series of conversations with data contributors and the Gen3 \u201cData Experience\u201d group. For example, if a data commons was designed to accept gene/genome sequences, in order to facilitate analysis of genetic data across studies submitted by different groups, a node called \"submitted aligned reads\" may require the following metadata properties (MTDE) labeled as \"Required: Yes\":","title":"Minimum Technical Data Elements (MTDE)"},{"location":"appendices/proxy-whitelist/","text":"Working with the proxy and whitelists \uf0c1 Working with the Proxy To prevent unauthorized traffic, the Gen3 VPC utilizes a proxy. If you are using one of the custom VMs setup, there is already a line in your .bashrc file to handle traffic requests. export http_proxy=http://cloud-proxy.internal.io:3128 export https_proxy=$http_proxy Alternatively, if you have a different service or a tool that needs to call out, you can set the proxy with each command. https_proxy=https://cloud-proxy.internal.io:3128 aws s3 ls s3://gen3-data/ --profile profilename Whitelists Additionally, to aid Gen3 Commons security, tool installation from outside sources is managed through a whitelist. If you have problems installing a tool you need for your work, contact and with a list of any sites you might wish to install tools from. After passing a security review, these can be added to the whitelist to facilitate access.","title":"Working with the Proxy and Whitelist"},{"location":"appendices/proxy-whitelist/#working-with-the-proxy-and-whitelists","text":"","title":"Working with the proxy and whitelists"},{"location":"appendices/template-tsvs/","text":"Template TSVs for metadata submission \uf0c1 Here are some sample TSV file templates for example nodes in a Gen3 data dictionary. You should look at \"Dictionary\" section of your Gen3 data commons to find the complete list of nodes and their template TSVs. study.tsv case.tsv biospecimen.tsv sample.tsv aliquot.tsv analyte.tsv","title":"Template TSVs"},{"location":"appendices/template-tsvs/#template-tsvs-for-metadata-submission","text":"Here are some sample TSV file templates for example nodes in a Gen3 data dictionary. You should look at \"Dictionary\" section of your Gen3 data commons to find the complete list of nodes and their template TSVs. study.tsv case.tsv biospecimen.tsv sample.tsv aliquot.tsv analyte.tsv","title":"Template TSVs for metadata submission"},{"location":"appendices/timepoints/","text":"Managing timepoints in a submission \uf0c1 Some elements of submitted datasets could be related to each other in time. To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates. Every other date field is anchored by the \"index_date\" in the \"case\" node. In this field you can have things like \"Study Enrollment\" or \"Diagnosis\". Study the case node in the dictionary for more information on the index_date field: https://data.Gen3.org/dd/case Examples of submissions using multiple date times Patient A enrolls in a study on July 1, and has a sample taken on July 10. For patient A you would report: case.index_date = \"Study Enrollment\" biospecimen.days_to_procurement = July 10 - July 1 = 9 Alternatively if they were diagnosed 2 years before the study began and you wanted to use that as the index_date nothing is stopping you: case.index_date = \"Diagnosis\" biospecimen.days_to_procurment = July 10, 2017 - July 1, 2015 = 739 Negative Dates Days to values can also be negative. If you have an index_date event that occurs after the event, you would present those days_to values as negative. If Patient A had a biospecimen taken when they were initially diagnosed: case.index_date = \"Study Enrollment\" biospecimen.days_to_procurement = July 10, 2015 - July 1, 2017 = -721 No Time Series The days_to_procurement and days_to_collection are required fields. If you do not have any data for these, we allow escape values of \"Unknown\" and \"Not Applicable\". Please use \"Unknown\" in the instances where you have established a time series but are unable to pin down the date of the event. Use \"Not Applicable\" if you do not have a time series at all.","title":"Managing Timepoints"},{"location":"appendices/timepoints/#managing-timepoints-in-a-submission","text":"Some elements of submitted datasets could be related to each other in time. To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates. Every other date field is anchored by the \"index_date\" in the \"case\" node. In this field you can have things like \"Study Enrollment\" or \"Diagnosis\". Study the case node in the dictionary for more information on the index_date field: https://data.Gen3.org/dd/case","title":"Managing timepoints in a submission"},{"location":"data-access/1-send-cred/","text":"1. Send credentials and get welcome email \uf0c1 Send SSH Key and Oauth to Gen3 commons team. \uf0c1 To access the VPC, users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to Gen3-support@datacommons.io. NOTE: Do not send your private ssh key. This is confidential and should never be shared with anyone. The pubkey will be used to access the login node and any VMs setup for the user. The email will be used to permit access to: Bionimbus to receive s3 data storage credentials data.Gen3.org for browser based querying of metadata. NOTE: for Gen3 members that were also their organization's contact for data submission, you already have access to your s3 credentials and data.Gen3.org. Just send your pubkey. I'm not familiar with SSH - how do I generate a keypair? \uf0c1 Github has a very nice tutorial with step by step instructions for Mac, Windows (users with gitbash installed), and Linux users. If you're new to using SSH we'd recommend reviewing the links: About SSH Checking for Existing SSH Keys * Generating a new SSH key and adding it to the SSH Agent NOTE: For Windows users, we recommend installing Git for Windows and using the Git Bash feature to interact on the command line, manage ssh keys, and s3 credentials. Receive a welcome email \uf0c1 Gen3 members with the appropriate signed legal documents will be sent an email that gives the following unique information: username (this is used to ssh to the VPC login node) - eg: ssh -A username @34.197.164.18 an IP associated with your new VM","title":"1. Send credentials and get welcome email"},{"location":"data-access/1-send-cred/#1-send-credentials-and-get-welcome-email","text":"","title":"1. Send credentials and get welcome email"},{"location":"data-access/1-send-cred/#send-ssh-key-and-oauth-to-gen3-commons-team","text":"To access the VPC, users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to Gen3-support@datacommons.io. NOTE: Do not send your private ssh key. This is confidential and should never be shared with anyone. The pubkey will be used to access the login node and any VMs setup for the user. The email will be used to permit access to: Bionimbus to receive s3 data storage credentials data.Gen3.org for browser based querying of metadata. NOTE: for Gen3 members that were also their organization's contact for data submission, you already have access to your s3 credentials and data.Gen3.org. Just send your pubkey.","title":"Send SSH Key and Oauth to Gen3 commons team."},{"location":"data-access/1-send-cred/#im-not-familiar-with-ssh-how-do-i-generate-a-keypair","text":"Github has a very nice tutorial with step by step instructions for Mac, Windows (users with gitbash installed), and Linux users. If you're new to using SSH we'd recommend reviewing the links: About SSH Checking for Existing SSH Keys * Generating a new SSH key and adding it to the SSH Agent NOTE: For Windows users, we recommend installing Git for Windows and using the Git Bash feature to interact on the command line, manage ssh keys, and s3 credentials.","title":"I'm not familiar with SSH - how do I generate a keypair?"},{"location":"data-access/1-send-cred/#receive-a-welcome-email","text":"Gen3 members with the appropriate signed legal documents will be sent an email that gives the following unique information: username (this is used to ssh to the VPC login node) - eg: ssh -A username @34.197.164.18 an IP associated with your new VM","title":"Receive a welcome email"},{"location":"data-access/2-ssh/","text":"2. SSH to Virtual Machine: config \uf0c1 How will I access the Login Node and my Virtual Machine (VM)? \uf0c1 Gen3 Commons users will login to the Virtual Private Cloud (VPC) headnode, then hop over to their analysis VM. For more information on the VPC, visit: https://www.synapse.org/#!Synapse:syn8011461/wiki/414186 In your welcome email you received your username and your vm. In order to access your VM, you first must access the VPC login node. This configuration helps ensure the security of the BloodPAC commons by having your VM in a private subnet. Using the credentials from your welcome email this can be done in the following order: SSH to login node: ssh -A username @34.197.164.18 SSH from login node to your VM: ssh -A ubuntu@ my_VM_ip NOTE 1: 34.197.164.18 is the IP for the login node. This is unlikely to change. NOTE 2: the -A flag forwards all the keys in your key chain. For more details on managing SSH keys, check the guides linked in the previous step . NOTE 3: You can't login to your analysis VM (in the private subnet) without first logging in to the login node (in the public subnet). Advanced users can manage these connections however they see fit. For other users, we recommend updating your SSH config file so you can setup a 'multihop' ssh tunnel. To 'multihop' in this context is to use a single command to get to your VM. What follows are instructions for updating your .ssh/config file to get to your VM in a single command. Setting up an ssh config for 'multihop' \uf0c1 To start, go to your .ssh directory in your laptop home directory. cd ~/.ssh If this directory does not exist, make one. mkdir -p ~/.ssh cd ~/.ssh Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]: vim config In this file you can specify various hosts for access via ssh. Your host for the BPA head node should look something like this: Host BPA Hostname 34.197.164.18 User YOUR_USERNAME IdentityFile /path/to/YOUR_CREDFILE ForwardAgent yes The username will be provided to you by the BloodPAC support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the BloodPAC head-node using this host: ssh BPA Exit the BPA head node and return to your local machine. Back in your config file, add a new host for the BPA submission VM: Host analysis Hostname YOUR_VM_IP User ubuntu ProxyCommand ssh -q -AXY BPA -W %h:%p Once again you will receive the Hostname IP from the BloodPAC support team in step 4. This host will route you through the BPA head node and take you directly to your personal BloodPAC analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM: ``` ssh analysis ```` If you've done everything correctly, you should now be in the analysis VM.","title":"2. SSH to Virtual Machine: config"},{"location":"data-access/2-ssh/#2-ssh-to-virtual-machine-config","text":"","title":"2. SSH to Virtual Machine: config"},{"location":"data-access/2-ssh/#how-will-i-access-the-login-node-and-my-virtual-machine-vm","text":"Gen3 Commons users will login to the Virtual Private Cloud (VPC) headnode, then hop over to their analysis VM. For more information on the VPC, visit: https://www.synapse.org/#!Synapse:syn8011461/wiki/414186 In your welcome email you received your username and your vm. In order to access your VM, you first must access the VPC login node. This configuration helps ensure the security of the BloodPAC commons by having your VM in a private subnet. Using the credentials from your welcome email this can be done in the following order: SSH to login node: ssh -A username @34.197.164.18 SSH from login node to your VM: ssh -A ubuntu@ my_VM_ip NOTE 1: 34.197.164.18 is the IP for the login node. This is unlikely to change. NOTE 2: the -A flag forwards all the keys in your key chain. For more details on managing SSH keys, check the guides linked in the previous step . NOTE 3: You can't login to your analysis VM (in the private subnet) without first logging in to the login node (in the public subnet). Advanced users can manage these connections however they see fit. For other users, we recommend updating your SSH config file so you can setup a 'multihop' ssh tunnel. To 'multihop' in this context is to use a single command to get to your VM. What follows are instructions for updating your .ssh/config file to get to your VM in a single command.","title":"How will I access the Login Node and my Virtual Machine (VM)?"},{"location":"data-access/2-ssh/#setting-up-an-ssh-config-for-multihop","text":"To start, go to your .ssh directory in your laptop home directory. cd ~/.ssh If this directory does not exist, make one. mkdir -p ~/.ssh cd ~/.ssh Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]: vim config In this file you can specify various hosts for access via ssh. Your host for the BPA head node should look something like this: Host BPA Hostname 34.197.164.18 User YOUR_USERNAME IdentityFile /path/to/YOUR_CREDFILE ForwardAgent yes The username will be provided to you by the BloodPAC support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the BloodPAC head-node using this host: ssh BPA Exit the BPA head node and return to your local machine. Back in your config file, add a new host for the BPA submission VM: Host analysis Hostname YOUR_VM_IP User ubuntu ProxyCommand ssh -q -AXY BPA -W %h:%p Once again you will receive the Hostname IP from the BloodPAC support team in step 4. This host will route you through the BPA head node and take you directly to your personal BloodPAC analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM: ``` ssh analysis ```` If you've done everything correctly, you should now be in the analysis VM.","title":"Setting up an ssh config for 'multihop'"},{"location":"data-access/3-access-data/","text":"3. Access \"raw\" data storage from Virtual Machine \uf0c1 Add s3 'raw' data storage credentials to your VM \uf0c1 Now you'll need to add your storage credentials to your analysis VM. Details on getting your credentials from the Bionimbus storage portal are outlined in the data contribution section of the wiki. If you are only accessing data and did not contribute, please follow those directions to acquire your keys using the Oauth you provided in Step 1 of the Data Access section. If you did contribute data you should use an existing key. They will still have \"read/write\" permission to your project folder, but will also have permission to \"read\" other data in the BloodPAC commons. If you submitted multiple projects and have multiple keys, all will have the same \"read\" permissions for data - it only matters which one you pick if you still intend to write data to your project folder from your VM. As a reminder, the command to setup a profile is: aws configure --profile CREATE YOUR PROFILE NAME Example: Configure an S3 profile in your VM \uf0c1 ${image?fileName=S3%5FVM%5FprofileLoop%2Egif align=Center responsive=true} Review contents of the Gen3 Commons \uf0c1 You can now review the 'raw' data folders in the Gen3 object storage. aws s3 ls s3://commons-data/ --profile profilename To peek inside a folder: aws s3 ls s3://commons-data/ foldername / --profile profilename You can use other commands to pull files from the s3 object storage into your VM. If you're not familiar with AWScli commands, we recommend reviewing the high-level docs or the complete documentation . NOTE: Remember, that since your access is read only (except any projects you've submitted associated with your keys), you will only be able to read, not write to the project folders in the commons. NOTE2: If you are using keys with write access to your project folders in the commons, be VERY CAREFUL. Don't delete any data you'll have to resubmit later. Ready to work! \uf0c1 You're ready to use whatever tools you wish to analyze data in the commons within your VM. For requests for alternative configurations, analysis storage, or other needs please contact gen3-support@datacommons.io. For an example of how you could use a Jupyter Notebook to run analysis in the browser on your local computer, please continue on to the next section . There are lots of good examples that may be useful to you.","title":"3. Access \"raw\" data storage from Virtual Machine"},{"location":"data-access/3-access-data/#3-access-raw-data-storage-from-virtual-machine","text":"","title":"3. Access \"raw\" data storage from Virtual Machine"},{"location":"data-access/3-access-data/#add-s3-raw-data-storage-credentials-to-your-vm","text":"Now you'll need to add your storage credentials to your analysis VM. Details on getting your credentials from the Bionimbus storage portal are outlined in the data contribution section of the wiki. If you are only accessing data and did not contribute, please follow those directions to acquire your keys using the Oauth you provided in Step 1 of the Data Access section. If you did contribute data you should use an existing key. They will still have \"read/write\" permission to your project folder, but will also have permission to \"read\" other data in the BloodPAC commons. If you submitted multiple projects and have multiple keys, all will have the same \"read\" permissions for data - it only matters which one you pick if you still intend to write data to your project folder from your VM. As a reminder, the command to setup a profile is: aws configure --profile CREATE YOUR PROFILE NAME","title":"Add s3 'raw' data storage credentials to your VM"},{"location":"data-access/3-access-data/#example-configure-an-s3-profile-in-your-vm","text":"${image?fileName=S3%5FVM%5FprofileLoop%2Egif align=Center responsive=true}","title":"Example:  Configure an S3 profile in your VM"},{"location":"data-access/3-access-data/#review-contents-of-the-gen3-commons","text":"You can now review the 'raw' data folders in the Gen3 object storage. aws s3 ls s3://commons-data/ --profile profilename To peek inside a folder: aws s3 ls s3://commons-data/ foldername / --profile profilename You can use other commands to pull files from the s3 object storage into your VM. If you're not familiar with AWScli commands, we recommend reviewing the high-level docs or the complete documentation . NOTE: Remember, that since your access is read only (except any projects you've submitted associated with your keys), you will only be able to read, not write to the project folders in the commons. NOTE2: If you are using keys with write access to your project folders in the commons, be VERY CAREFUL. Don't delete any data you'll have to resubmit later.","title":"Review contents of the Gen3 Commons"},{"location":"data-access/3-access-data/#ready-to-work","text":"You're ready to use whatever tools you wish to analyze data in the commons within your VM. For requests for alternative configurations, analysis storage, or other needs please contact gen3-support@datacommons.io. For an example of how you could use a Jupyter Notebook to run analysis in the browser on your local computer, please continue on to the next section . There are lots of good examples that may be useful to you.","title":"Ready to work!"},{"location":"data-access/access-overview/","text":"Overview of Data Access \uf0c1 Once the appropriate agreements are in place, the Gen3 Data will be available to members through a VPC . Instructions for data access include: Send credentials, get welcome email SSH to VM: config Access \"raw\" data storage from VM Use Jupyter notebook for analysis","title":"Overview of Data Access"},{"location":"data-access/access-overview/#overview-of-data-access","text":"Once the appropriate agreements are in place, the Gen3 Data will be available to members through a VPC . Instructions for data access include: Send credentials, get welcome email SSH to VM: config Access \"raw\" data storage from VM Use Jupyter notebook for analysis","title":"Overview of Data Access"},{"location":"data-contribution/1-legal/","text":"1. Review and sign legal agreement \uf0c1 To use a Gen3 commons, please review and sign the following agreements and return them to info@gen3.org. The most current versions (and past versions for transparency) of all of the below documents can be found at: https://www.gen3.org/data-governance/ Data Contributor Agreement (DCA) Data Services/Use Agreement (DUA) If you only wish to contribute data, you do not need to sign the DUA. These documents may also reference the: Privacy and Security Agreement Intellectual Property Rights (IPR) Policy","title":"1. Review and sign legal agreement"},{"location":"data-contribution/1-legal/#1-review-and-sign-legal-agreement","text":"To use a Gen3 commons, please review and sign the following agreements and return them to info@gen3.org. The most current versions (and past versions for transparency) of all of the below documents can be found at: https://www.gen3.org/data-governance/ Data Contributor Agreement (DCA) Data Services/Use Agreement (DUA) If you only wish to contribute data, you do not need to sign the DUA. These documents may also reference the: Privacy and Security Agreement Intellectual Property Rights (IPR) Policy","title":"1. Review and sign legal agreement"},{"location":"data-contribution/2-data-inventory/","text":"2. Complete the data inventory form \uf0c1 Prepare a pre-submission data inventory form Having this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form.","title":"2. Complete the data inventory form"},{"location":"data-contribution/2-data-inventory/#2-complete-the-data-inventory-form","text":"Prepare a pre-submission data inventory form Having this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form.","title":"2. Complete the data inventory form"},{"location":"data-contribution/3-api-creds/","text":"3. Receive project name and API access credentials \uf0c1 Once you have completed the data inventory form, link to step , you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud (VPC) headnode), and instructions to access the metadata submission portal and an object storage for your project. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata, link to metadata submission for your experiment nodes. Project name example \uf0c1 mycompanyname _P0001_T1 mycompanyname _P0002_T1 mycompanyname _P0001_T2 Breakdown: \uf0c1 \" \" identifies the submitter organization \"P000x\" identifies the submission number for said organization in said train \"Tx\" identifies the train number NOTE: the Gen3 data submission calendar is broken up into different trains. NOTE2: Your project folder will have a prefix appended to it to identify the commons. eg - BloodPAC (BPA): BPA_ mycompanyname _P0001_T1","title":"3. Receive project name and API access credentials"},{"location":"data-contribution/3-api-creds/#3-receive-project-name-and-api-access-credentials","text":"Once you have completed the data inventory form, link to step , you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud (VPC) headnode), and instructions to access the metadata submission portal and an object storage for your project. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata, link to metadata submission for your experiment nodes.","title":"3. Receive project name and API access credentials"},{"location":"data-contribution/3-api-creds/#project-name-example","text":"mycompanyname _P0001_T1 mycompanyname _P0002_T1 mycompanyname _P0001_T2","title":"Project name example"},{"location":"data-contribution/3-api-creds/#breakdown","text":"\" \" identifies the submitter organization \"P000x\" identifies the submission number for said organization in said train \"Tx\" identifies the train number NOTE: the Gen3 data submission calendar is broken up into different trains. NOTE2: Your project folder will have a prefix appended to it to identify the commons. eg - BloodPAC (BPA): BPA_ mycompanyname _P0001_T1","title":"Breakdown:"},{"location":"data-contribution/4-prep-meta/","text":"4. Prepare metadata that fits the data model \uf0c1 Overview \uf0c1 Gen3 data contributors will need to prepare metadata for their submission in tab-separated value (tsv) files, login to a portal provided for submission, and upload and validate their metadata submission. This is simultaneously the most challenging and crucial part of contributing data to a Gen3 commons. This page details the preparation and ordering of the tsvs that will be submitted. The next two pages cover submission virtual machine (VM) access and uploading and validating your metadata tsv submission. Review and understand the data model \uf0c1 Now that you have your project name, you can begin building out the rest of your metadata. Reference the most recent graph model of your commons at https://www.Gen3.org/data-group/ to help guide you on the necessary components in your submission. As you can see, from project you build up to experiment then to case and so on. For the properties that are allowed within this submission, please take some time to read through the dictionary schemas. The raw schema can be found at: . Descriptions for each property as well as the valid submission values can be found in those schemas. Once you have access to submission portal, link to submission portal doc , we recommend using the Data Dictionary Viewer, link to dict viewer to review the schema and determine which properties best describe your submission. This tool will help you understand the field types, requirements, and node dependencies for your submission. Create your TSVs \uf0c1 It may be helpful to think of each TSV as a node in the graph of the data model. Each node can have multiple components, for example a project could have multiple experiments in a node. * Blank TSV templates can be found at https://www.synapse.org/#!Synapse:syn10268802 * Note there are wiki pages associated with each potential tsv or node in the templates. They show example fields and information about data provenance. * Field types and limitations can be cleaned from a careful read of the associated yaml files at: https://github.com/occ-data/bpadictionary/tree/develop/gdcdictionary/schemas Determine Submission Order \uf0c1 Before we discuss the actual submission process, we must first mention that the files must be submitted in a specific order. Once again referring back to the graph model at https://www.bloodpac.org/data-group, you cannot submit a node without submitting the nodes to which it points. If you submit a file out of order, the validator will reject the submission on the basis that the dependency you point to (e.g. the read_groups.submitter_id in assay_result.tsv will be pointing to a node that doesn\u2019t exist) is not present. The Data Dictionary viewer can help you determine these dependencies. Sample Diagram of TSV Submission Order \uf0c1 While this diagram represents an earlier version of the BloodPAC data model, the required submission logic for current versions of the model will be very similar. ${image?fileName=blood%5Fatlas%5F010617%5Fmarkup%2Epng align=None responsive=true}","title":"4. Prepare metadata that fits the data model"},{"location":"data-contribution/4-prep-meta/#4-prepare-metadata-that-fits-the-data-model","text":"","title":"4. Prepare metadata that fits the data model"},{"location":"data-contribution/4-prep-meta/#overview","text":"Gen3 data contributors will need to prepare metadata for their submission in tab-separated value (tsv) files, login to a portal provided for submission, and upload and validate their metadata submission. This is simultaneously the most challenging and crucial part of contributing data to a Gen3 commons. This page details the preparation and ordering of the tsvs that will be submitted. The next two pages cover submission virtual machine (VM) access and uploading and validating your metadata tsv submission.","title":"Overview"},{"location":"data-contribution/4-prep-meta/#review-and-understand-the-data-model","text":"Now that you have your project name, you can begin building out the rest of your metadata. Reference the most recent graph model of your commons at https://www.Gen3.org/data-group/ to help guide you on the necessary components in your submission. As you can see, from project you build up to experiment then to case and so on. For the properties that are allowed within this submission, please take some time to read through the dictionary schemas. The raw schema can be found at: . Descriptions for each property as well as the valid submission values can be found in those schemas. Once you have access to submission portal, link to submission portal doc , we recommend using the Data Dictionary Viewer, link to dict viewer to review the schema and determine which properties best describe your submission. This tool will help you understand the field types, requirements, and node dependencies for your submission.","title":"Review and understand the data model"},{"location":"data-contribution/4-prep-meta/#create-your-tsvs","text":"It may be helpful to think of each TSV as a node in the graph of the data model. Each node can have multiple components, for example a project could have multiple experiments in a node. * Blank TSV templates can be found at https://www.synapse.org/#!Synapse:syn10268802 * Note there are wiki pages associated with each potential tsv or node in the templates. They show example fields and information about data provenance. * Field types and limitations can be cleaned from a careful read of the associated yaml files at: https://github.com/occ-data/bpadictionary/tree/develop/gdcdictionary/schemas","title":"Create your TSVs"},{"location":"data-contribution/4-prep-meta/#determine-submission-order","text":"Before we discuss the actual submission process, we must first mention that the files must be submitted in a specific order. Once again referring back to the graph model at https://www.bloodpac.org/data-group, you cannot submit a node without submitting the nodes to which it points. If you submit a file out of order, the validator will reject the submission on the basis that the dependency you point to (e.g. the read_groups.submitter_id in assay_result.tsv will be pointing to a node that doesn\u2019t exist) is not present. The Data Dictionary viewer can help you determine these dependencies.","title":"Determine Submission Order"},{"location":"data-contribution/4-prep-meta/#sample-diagram-of-tsv-submission-order","text":"While this diagram represents an earlier version of the BloodPAC data model, the required submission logic for current versions of the model will be very similar. ${image?fileName=blood%5Fatlas%5F010617%5Fmarkup%2Epng align=None responsive=true}","title":"Sample Diagram of TSV Submission Order"},{"location":"data-contribution/5-access-meta/","text":"5. Access metadata that fits the data model \uf0c1 What is the metadata submission portal? \uf0c1 The metadata submission portal is secure environment for submitting the tsv metadata files associated with your project submission, querying the metadata associated with your project and others, and using the Data Dictionary Viewer to understand fields. You will be able to login with your OAuth and you will have access update or edit the metadata associated with your submission by adding tsv files or \"nodes\" to the graph. Where is the metadata submission portal? \uf0c1 Links to Gen3 Commons Submission portals: BloodPAC","title":"5. Access metadata that fits the data model"},{"location":"data-contribution/5-access-meta/#5-access-metadata-that-fits-the-data-model","text":"","title":"5. Access metadata that fits the data model"},{"location":"data-contribution/5-access-meta/#what-is-the-metadata-submission-portal","text":"The metadata submission portal is secure environment for submitting the tsv metadata files associated with your project submission, querying the metadata associated with your project and others, and using the Data Dictionary Viewer to understand fields. You will be able to login with your OAuth and you will have access update or edit the metadata associated with your submission by adding tsv files or \"nodes\" to the graph.","title":"What is the metadata submission portal?"},{"location":"data-contribution/5-access-meta/#where-is-the-metadata-submission-portal","text":"Links to Gen3 Commons Submission portals: BloodPAC","title":"Where is the metadata submission portal?"},{"location":"data-contribution/6-submit-meta/","text":"6. Submit and validate project metadata tsvs \uf0c1 Begin your submission \uf0c1 From the submission portal select the project for which you wish to submit metadata. Remembering the order in which you need to submit your tsvs (see step 5, Determine Submission Order for details) begin your submission by uploading or copying in your first tsv (likely \"experiment.tsv\"). NOTE: If you would prefer submitting nodes in json to tsv the submission portal also accepts json. To get you started, the first node - \"project\" has already been created for you. Now you should see a lot of details about the submission process. At the very bottom, you should be able to immediately get a grasp of how the submission went with the following fields: {'entity_error_count': 0, 'message': 'Transaction successful.', 'success': True, 'transaction_id': 403, 'transactional_error_count': 0, 'transactional_errors': [], 'updated_entity_count': 40} Specifically, the message and success fields should provide you with whether your submission was valid and went through. If you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes: {'action': 'update', 'errors': [], 'id': 'asdf21as-2q4a-2563-213k-8dn4kg8dsb3j', 'related_cases': [], 'type': 'case', 'unique_keys': [{'project_id': 'bpa-MyGroup_P0001_T1' 'submitter_id': u'BPA_MG_P0001_EX1_C1'}], 'valid': True, 'warnings': []}, On a successful submission, the API will return something like above. The action can be used to identify if the node was created new or just updated. Other useful information includes the id for the node. This is the UUID for the submission and is unique for the node throughout the entirety of the Gen3 Commons. The other unique_key provided is the tuple project_id and submitter_id; another way of saying that the submitter_id combined with the project_id is a universal identifier for this node. Troubleshooting and finishing your submission \uf0c1 If, against all odds, your submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission. The best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what tripped our validation check. The most common problems are simple issues such as spelling errors or mislabeled fields. Other errors include the one I mentioned earlier about submitting out of order and errors from not adhering to valid values as defined in our dictionary. If you don't receive any output it usually means your TSV is invalid and must be edited. Submission Failure Example \uf0c1 ALTERNATIVE in BETA: Form submission \uf0c1 Using tsvs allows users to bulk submit their metadata. This is ultimately a much faster process for users with larger datasets. If you wish, you can also use form submission to insert properties into a node. NOTE: if you use the same submitter ID, and submit again to the same node, it will overwrite properties. Be sure to change it should you choose to use form submission multiple times on a given node. NOTE2: it is not currently possible to remove values submitted in error using the form submission method. ${image?fileName=formSubmission%2Egif align=None scale=100 responsive=true} Query the API \uf0c1 If helpful you can use query the API in the submission portal to confirm or correct your submission, to delete nodes, or to submit json queries to learn more about your submission or all the data in the commons. To learn more visit the API section of the wiki: https://www.synapse.org/#!Synapse:syn8011461/wiki/415875 Check the commons matrices \uf0c1 Read \"What's an example of the API at work?\" to learn more about how the data matrices work. Suffice to say, you can check them hourly to see relevant metadata you submit appear! Provide feedback \uf0c1 You may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the Gen3 support team at gen3-support@occ-data.org. We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values. Let us know submission is complete \uf0c1 Please contact the Gen3 support team to let us know when your submission is complete. Submission FAQ \uf0c1 How do I know my submission was accepted? \uf0c1 You will see this within the json output: 'message': 'Transaction successful.', 'success': True, How can I learn more about the elements of my existing submission? \uf0c1 When you are viewing a project, there is a \"browse nodes\" feature. From here you can download, view, or completely delete the tsvs associated with any project you have write access to. What happens if i need to resubmit a tsv that was already accepted? \uf0c1 When you resubmit (aka submit to a node with the same submitter id), you will update the existing node. For example, if you submit a sample with sample_type , composition , and tube_type , and you later realize that the tube type was wrong for that submission, if you were to resubmit a tsv that has just the submitter_id and tube_type it will overwrite ONLY the tube type. The sample_type and composition from the previous submission will still be in the database. What this means is that if you had previously submitted something you DON'T want, like say you realize that you accidentally submitted the tube_type as method_of_sample_procurement , simply renaming the header in your TSV would not overwrite the existing data in the node. You would need to submit null/empty values for method_of_sample_procurement to get rid of it. I was part of the very first submission group, in Nov/Dec 2016 when we added TSVs directly into an object store. Where is the project.submitter_id field? \uf0c1 projects.submitter_id has become projects.code now. SO: EXAMPLE: program.name = 'bpa' project.code ='MyOrg_P0001_T1' * project_id = program.name + '-' + project.code = 'bpa-MyOrg_P0001_T1'","title":"6. Submit and validate project metadata tsvs"},{"location":"data-contribution/6-submit-meta/#6-submit-and-validate-project-metadata-tsvs","text":"","title":"6. Submit and validate project metadata tsvs"},{"location":"data-contribution/6-submit-meta/#begin-your-submission","text":"From the submission portal select the project for which you wish to submit metadata. Remembering the order in which you need to submit your tsvs (see step 5, Determine Submission Order for details) begin your submission by uploading or copying in your first tsv (likely \"experiment.tsv\"). NOTE: If you would prefer submitting nodes in json to tsv the submission portal also accepts json. To get you started, the first node - \"project\" has already been created for you. Now you should see a lot of details about the submission process. At the very bottom, you should be able to immediately get a grasp of how the submission went with the following fields: {'entity_error_count': 0, 'message': 'Transaction successful.', 'success': True, 'transaction_id': 403, 'transactional_error_count': 0, 'transactional_errors': [], 'updated_entity_count': 40} Specifically, the message and success fields should provide you with whether your submission was valid and went through. If you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes: {'action': 'update', 'errors': [], 'id': 'asdf21as-2q4a-2563-213k-8dn4kg8dsb3j', 'related_cases': [], 'type': 'case', 'unique_keys': [{'project_id': 'bpa-MyGroup_P0001_T1' 'submitter_id': u'BPA_MG_P0001_EX1_C1'}], 'valid': True, 'warnings': []}, On a successful submission, the API will return something like above. The action can be used to identify if the node was created new or just updated. Other useful information includes the id for the node. This is the UUID for the submission and is unique for the node throughout the entirety of the Gen3 Commons. The other unique_key provided is the tuple project_id and submitter_id; another way of saying that the submitter_id combined with the project_id is a universal identifier for this node.","title":"Begin your submission"},{"location":"data-contribution/6-submit-meta/#troubleshooting-and-finishing-your-submission","text":"If, against all odds, your submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission. The best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what tripped our validation check. The most common problems are simple issues such as spelling errors or mislabeled fields. Other errors include the one I mentioned earlier about submitting out of order and errors from not adhering to valid values as defined in our dictionary. If you don't receive any output it usually means your TSV is invalid and must be edited.","title":"Troubleshooting and finishing your submission"},{"location":"data-contribution/6-submit-meta/#submission-failure-example","text":"","title":"Submission Failure Example"},{"location":"data-contribution/6-submit-meta/#alternative-in-beta-form-submission","text":"Using tsvs allows users to bulk submit their metadata. This is ultimately a much faster process for users with larger datasets. If you wish, you can also use form submission to insert properties into a node. NOTE: if you use the same submitter ID, and submit again to the same node, it will overwrite properties. Be sure to change it should you choose to use form submission multiple times on a given node. NOTE2: it is not currently possible to remove values submitted in error using the form submission method. ${image?fileName=formSubmission%2Egif align=None scale=100 responsive=true}","title":"ALTERNATIVE in BETA:  Form submission"},{"location":"data-contribution/6-submit-meta/#query-the-api","text":"If helpful you can use query the API in the submission portal to confirm or correct your submission, to delete nodes, or to submit json queries to learn more about your submission or all the data in the commons. To learn more visit the API section of the wiki: https://www.synapse.org/#!Synapse:syn8011461/wiki/415875","title":"Query the API"},{"location":"data-contribution/6-submit-meta/#check-the-commons-matrices","text":"Read \"What's an example of the API at work?\" to learn more about how the data matrices work. Suffice to say, you can check them hourly to see relevant metadata you submit appear!","title":"Check the commons matrices"},{"location":"data-contribution/6-submit-meta/#provide-feedback","text":"You may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the Gen3 support team at gen3-support@occ-data.org. We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values.","title":"Provide feedback"},{"location":"data-contribution/6-submit-meta/#let-us-know-submission-is-complete","text":"Please contact the Gen3 support team to let us know when your submission is complete.","title":"Let us know submission is complete"},{"location":"data-contribution/6-submit-meta/#submission-faq","text":"","title":"Submission FAQ"},{"location":"data-contribution/6-submit-meta/#how-do-i-know-my-submission-was-accepted","text":"You will see this within the json output: 'message': 'Transaction successful.', 'success': True,","title":"How do I know my submission was accepted?"},{"location":"data-contribution/6-submit-meta/#how-can-i-learn-more-about-the-elements-of-my-existing-submission","text":"When you are viewing a project, there is a \"browse nodes\" feature. From here you can download, view, or completely delete the tsvs associated with any project you have write access to.","title":"How can I learn more about the elements of my existing submission?"},{"location":"data-contribution/6-submit-meta/#what-happens-if-i-need-to-resubmit-a-tsv-that-was-already-accepted","text":"When you resubmit (aka submit to a node with the same submitter id), you will update the existing node. For example, if you submit a sample with sample_type , composition , and tube_type , and you later realize that the tube type was wrong for that submission, if you were to resubmit a tsv that has just the submitter_id and tube_type it will overwrite ONLY the tube type. The sample_type and composition from the previous submission will still be in the database. What this means is that if you had previously submitted something you DON'T want, like say you realize that you accidentally submitted the tube_type as method_of_sample_procurement , simply renaming the header in your TSV would not overwrite the existing data in the node. You would need to submit null/empty values for method_of_sample_procurement to get rid of it.","title":"What happens if i need to resubmit a tsv that was already accepted?"},{"location":"data-contribution/6-submit-meta/#i-was-part-of-the-very-first-submission-group-in-novdec-2016-when-we-added-tsvs-directly-into-an-object-store-where-is-the-projectsubmitter_id-field","text":"projects.submitter_id has become projects.code now. SO: EXAMPLE: program.name = 'bpa' project.code ='MyOrg_P0001_T1' * project_id = program.name + '-' + project.code = 'bpa-MyOrg_P0001_T1'","title":"I was part of the very first submission group, in Nov/Dec 2016 when we added TSVs directly into an object store.   Where is the project.submitter_id field?"},{"location":"data-contribution/7-s3-cred/","text":"7. Get and configure s3 data storage credentials \uf0c1 Overview \uf0c1 Now that you've successfully submitted and validated your project metadata, it's time to upload your 'raw' data to the Gen3 Commons. The Gen3 commons utilize object storage . This page details how users gain and manage the credentials to access a project folder. The following page will detail submitting data to the project folder. Why should we use command line to submit data instead of some kind of website? Because for transfer of large files or many files, there can be time-out issues, encryption issues, or corruption issues. Using the command line ensures secure and complete transfer. NOTE: if you only have small files and are uncomfortable operating on the command line, you may want to try using a GUI tool like Cyberduck to connect to S3 and manage your upload instead. Obtain S3 credentials \uf0c1 Go to https://bionimbus-pdc.opensciencedatacloud.org/storage/ Select and use the authentication method you gave in the data inventory form Download access keys by selecting the appropriate button next to the information for your username, e.g., under \"Available BPA Datasets\". The button is labeled \u201cGenerate S3 credential.\u201d See Figure 1. Credentials appear as comma-separated values including an access key and a secret key. The secret key should remain secret. Do not share these keys. NOTE: Do not share the s3 credentials you gain below with anyone. If you need to obtain new credentials, repeat steps (2.a) through (2.c). The button will now say \u201cRotate key\u201d button, which will deactivate previous credentials and provide new ones. Figure 1 \uf0c1 ${image?fileName=bionimbussubmit%2Epng align=None responsive=true} KNOWN ISSUE: Safari will not provide S3 credentials on the first try. After generating keys once, press the \u201cRotate key\u201d button. Install AWS CLI \uf0c1 Mac OSX or Linux/Unix \uf0c1 Open your \u201cTerminal\u201d application and type the following command: sudo apt-get install awscli or sudo easy_install awscli If prompted, enter your device\u2019s password. Windows \uf0c1 Download and run one of the windows installers at: https://aws.amazon.com/cli/ Configure AWS CLI with the downloaded project credentials \uf0c1 In your \u201cTerminal\u201d application, type the line below, replacing \u201cCREATE YOUR PROFILE NAME\u201d with a profile name you create. aws configure --profile CREATE YOUR PROFILE NAME Pro-tip: Profile management will be very important as groups submit multiple projects. Your downloaded s3 credentials only give you access to the project folder for which they are created. If you are only submitting one project, you don't need profiles. If you are submitting multiple, you will want to carefully pick project names for each project. eg - P0001_T1 and P0002_T1. Some good sources on managing multiple profiles: OSX/Linux: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles Windows powershell: http://docs.aws.amazon.com/powershell/latest/userguide/specifying-your-aws-credentials.html After pressing Enter/Return, you will be prompted for your Access Key ID and Secret Access Key. These are the S3 credentials you downloaded earlier. Enter keys as prompted, pressing Enter/Return after each step. AWS Access Key ID [****************]: AWS Secret Access Key [****************]: Enter a default region name, as prompted - us-east-1. Press Enter/Return. Default region name [None]: us-east-1 Press return at the final prompt. Default output format [None]: You should now be able to see the folder names in the bpa-data bucket. aws s3 ls s3://bpa-data/ --profile profilename","title":"7. Get and configure s3 data storage credentials"},{"location":"data-contribution/7-s3-cred/#7-get-and-configure-s3-data-storage-credentials","text":"","title":"7. Get and configure s3 data storage credentials"},{"location":"data-contribution/7-s3-cred/#overview","text":"Now that you've successfully submitted and validated your project metadata, it's time to upload your 'raw' data to the Gen3 Commons. The Gen3 commons utilize object storage . This page details how users gain and manage the credentials to access a project folder. The following page will detail submitting data to the project folder. Why should we use command line to submit data instead of some kind of website? Because for transfer of large files or many files, there can be time-out issues, encryption issues, or corruption issues. Using the command line ensures secure and complete transfer. NOTE: if you only have small files and are uncomfortable operating on the command line, you may want to try using a GUI tool like Cyberduck to connect to S3 and manage your upload instead.","title":"Overview"},{"location":"data-contribution/7-s3-cred/#obtain-s3-credentials","text":"Go to https://bionimbus-pdc.opensciencedatacloud.org/storage/ Select and use the authentication method you gave in the data inventory form Download access keys by selecting the appropriate button next to the information for your username, e.g., under \"Available BPA Datasets\". The button is labeled \u201cGenerate S3 credential.\u201d See Figure 1. Credentials appear as comma-separated values including an access key and a secret key. The secret key should remain secret. Do not share these keys. NOTE: Do not share the s3 credentials you gain below with anyone. If you need to obtain new credentials, repeat steps (2.a) through (2.c). The button will now say \u201cRotate key\u201d button, which will deactivate previous credentials and provide new ones.","title":"Obtain S3 credentials"},{"location":"data-contribution/7-s3-cred/#figure-1","text":"${image?fileName=bionimbussubmit%2Epng align=None responsive=true} KNOWN ISSUE: Safari will not provide S3 credentials on the first try. After generating keys once, press the \u201cRotate key\u201d button.","title":"Figure 1"},{"location":"data-contribution/7-s3-cred/#install-aws-cli","text":"","title":"Install AWS CLI"},{"location":"data-contribution/7-s3-cred/#mac-osx-or-linuxunix","text":"Open your \u201cTerminal\u201d application and type the following command: sudo apt-get install awscli or sudo easy_install awscli If prompted, enter your device\u2019s password.","title":"Mac OSX or Linux/Unix"},{"location":"data-contribution/7-s3-cred/#windows","text":"Download and run one of the windows installers at: https://aws.amazon.com/cli/","title":"Windows"},{"location":"data-contribution/7-s3-cred/#configure-aws-cli-with-the-downloaded-project-credentials","text":"In your \u201cTerminal\u201d application, type the line below, replacing \u201cCREATE YOUR PROFILE NAME\u201d with a profile name you create. aws configure --profile CREATE YOUR PROFILE NAME Pro-tip: Profile management will be very important as groups submit multiple projects. Your downloaded s3 credentials only give you access to the project folder for which they are created. If you are only submitting one project, you don't need profiles. If you are submitting multiple, you will want to carefully pick project names for each project. eg - P0001_T1 and P0002_T1. Some good sources on managing multiple profiles: OSX/Linux: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles Windows powershell: http://docs.aws.amazon.com/powershell/latest/userguide/specifying-your-aws-credentials.html After pressing Enter/Return, you will be prompted for your Access Key ID and Secret Access Key. These are the S3 credentials you downloaded earlier. Enter keys as prompted, pressing Enter/Return after each step. AWS Access Key ID [****************]: AWS Secret Access Key [****************]: Enter a default region name, as prompted - us-east-1. Press Enter/Return. Default region name [None]: us-east-1 Press return at the final prompt. Default output format [None]: You should now be able to see the folder names in the bpa-data bucket. aws s3 ls s3://bpa-data/ --profile profilename","title":"Configure AWS CLI with the downloaded project credentials"},{"location":"data-contribution/8-upload/","text":"8. Upload \"raw\" data to object storage \uf0c1 Preparing your data \uf0c1 Data files such as BAMs, FASTQs, or PDFs should be uploaded directly to the object store. The metadata TSVs you prepared should not be submitted to the object store, as they have already been submitted via the API. Please prepare a single folder with all of submission files in the same directory. No sub directories. Uploading your data) \uf0c1 You can now upload all the files in the prepared folder on your local computer using the AWS CLI and the profiles you configured in step 7 . Below is the command to copy a folder filled with all your submission files on your local computer to your project folder in a commons. You need to change the name of /path/folder/ to the name of the path of the folder you want to upload. aws s3 cp --sse AES256 [/path/folder/] s3://bpa-data/[foldername] --recursive --profile [profilename] EXTRA: In an object store, a \"folder\" or \"dir\" can't exist with nothing in it. Thus, if you were to 'ls' before moving any files into it, you wouldn't see the project folder you have access to. In the example above you're essentially uploading all your data and \"creating a folder\" in a single step. Other useful commands and AWS CLI documentation can be found at: https://www.opensciencedatacloud.org/support/pdc.html and https://aws.amazon.com/cli/","title":"8. Upload \"raw\" data to object storage"},{"location":"data-contribution/8-upload/#8-upload-raw-data-to-object-storage","text":"","title":"8. Upload \"raw\" data to object storage"},{"location":"data-contribution/8-upload/#preparing-your-data","text":"Data files such as BAMs, FASTQs, or PDFs should be uploaded directly to the object store. The metadata TSVs you prepared should not be submitted to the object store, as they have already been submitted via the API. Please prepare a single folder with all of submission files in the same directory. No sub directories.","title":"Preparing your data"},{"location":"data-contribution/8-upload/#uploading-your-data","text":"You can now upload all the files in the prepared folder on your local computer using the AWS CLI and the profiles you configured in step 7 . Below is the command to copy a folder filled with all your submission files on your local computer to your project folder in a commons. You need to change the name of /path/folder/ to the name of the path of the folder you want to upload. aws s3 cp --sse AES256 [/path/folder/] s3://bpa-data/[foldername] --recursive --profile [profilename] EXTRA: In an object store, a \"folder\" or \"dir\" can't exist with nothing in it. Thus, if you were to 'ls' before moving any files into it, you wouldn't see the project folder you have access to. In the example above you're essentially uploading all your data and \"creating a folder\" in a single step. Other useful commands and AWS CLI documentation can be found at: https://www.opensciencedatacloud.org/support/pdc.html and https://aws.amazon.com/cli/","title":"Uploading your data)"},{"location":"data-contribution/contribution-overview/","text":"Project Submission Overview \uf0c1 Sign documents, fill out forms, get credentials Prepare and submit metadata tsvs to submission VM Prepare and submit full dataset w/o metadata tsvs to object storage. Steps to Contribute a Project to the Gen3 Commons \uf0c1 Review and sign legal Complete the data inventory form Receive project name and access credentials Prepare metadata that fits the data model Access metadata submission portal Submit and validate project metadata tsvs Get and configure s3 data storage credentials Upload \"raw\" data to object storage NOTE: Gen3 members are encouraged to submit multiple projects to the commons. To do so, repeat steps 2-8 above. Why Do Gen3 Commons Use a Data Model? \uf0c1 Having all participating members use the same data model: Allows for standardized metadata elements across a commons. Permits flexible and scaleable API generation based on data commons software that reads the data model schema. Lets users query the commons API so that an ecosystem of applications can be built. Helps automate the validation of submitted data. For the most current model, go to: https://github.com/uc-cdis/datadictionary For the most current graph of the model, go to: https://www.gen3.org/data-group/ If you have an submission element that you believe can't be described in the model, we'd be glad to work with you to find a home for the element or update the model. Questions? \uf0c1 Contact: For Legal: gen3-legal@occ-data.org For Submission/Access Questions: gen3-support@datacommons.io","title":"Project Submission Overview"},{"location":"data-contribution/contribution-overview/#project-submission-overview","text":"Sign documents, fill out forms, get credentials Prepare and submit metadata tsvs to submission VM Prepare and submit full dataset w/o metadata tsvs to object storage.","title":"Project Submission Overview"},{"location":"data-contribution/contribution-overview/#steps-to-contribute-a-project-to-the-gen3-commons","text":"Review and sign legal Complete the data inventory form Receive project name and access credentials Prepare metadata that fits the data model Access metadata submission portal Submit and validate project metadata tsvs Get and configure s3 data storage credentials Upload \"raw\" data to object storage NOTE: Gen3 members are encouraged to submit multiple projects to the commons. To do so, repeat steps 2-8 above.","title":"Steps to Contribute a Project to the Gen3 Commons"},{"location":"data-contribution/contribution-overview/#why-do-gen3-commons-use-a-data-model","text":"Having all participating members use the same data model: Allows for standardized metadata elements across a commons. Permits flexible and scaleable API generation based on data commons software that reads the data model schema. Lets users query the commons API so that an ecosystem of applications can be built. Helps automate the validation of submitted data. For the most current model, go to: https://github.com/uc-cdis/datadictionary For the most current graph of the model, go to: https://www.gen3.org/data-group/ If you have an submission element that you believe can't be described in the model, we'd be glad to work with you to find a home for the element or update the model.","title":"Why Do Gen3 Commons Use a Data Model?"},{"location":"data-contribution/contribution-overview/#questions","text":"Contact: For Legal: gen3-legal@occ-data.org For Submission/Access Questions: gen3-support@datacommons.io","title":"Questions?"},{"location":"demos/bloodpac-demo/","text":"BloodPAC DEMO: Using a Jupyter notebook for analysis \uf0c1 The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/bpa-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information. Running the notebook in your VM \uf0c1 After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c Port forwarding to your VM \uf0c1 Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 . Access the notebook in via browser \uf0c1 In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser. Example: Run Server, port forward, access notebook in browser \uf0c1 Review and follow the notebook \uf0c1 Credentials The notebook makes use of both a .secrets file that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the s3 profile you created to pull 'raw' data into your VM for analysis. The first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is. NOTE: If you have forgotten what you called your profile, you can always take a look at the credential file to review. From the VM run: vi ~/.aws/credentials . Jupyter Basics If you're not familiar with Jupyter notebooks, you have a few options to run the commands inside. You can review line by line (select cell - Shift+Enter ) or you can run all from the \"Kernel\" menu at the top of the browser. Shutting Down your Server When you're done working, we encourage you to shut down your Jupyter server via ctrl + c in the VM that's running it. You don't have to do this every time, but you should do it when you don't need it any more. VM Termination At this point in the Gen3 commons development, you should contact when you no longer need your VM active. Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.","title":"BloodPAC DEMO: Using a Jupyter notebook for analysis"},{"location":"demos/bloodpac-demo/#bloodpac-demo-using-a-jupyter-notebook-for-analysis","text":"The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/bpa-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information.","title":"BloodPAC DEMO: Using a Jupyter notebook for analysis"},{"location":"demos/bloodpac-demo/#running-the-notebook-in-your-vm","text":"After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c","title":"Running the notebook in your VM"},{"location":"demos/bloodpac-demo/#port-forwarding-to-your-vm","text":"Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 .","title":"Port forwarding to your VM"},{"location":"demos/bloodpac-demo/#access-the-notebook-in-via-browser","text":"In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.","title":"Access the notebook in via browser"},{"location":"demos/bloodpac-demo/#example-run-server-port-forward-access-notebook-in-browser","text":"","title":"Example:   Run Server, port forward, access notebook in browser"},{"location":"demos/bloodpac-demo/#review-and-follow-the-notebook","text":"","title":"Review and follow the notebook"},{"location":"old_verions/data-access-June2018/","text":"Data Access Overview The sponsor of a Gen3 data commons typically decides how users will access data in object storage. In some cases, approved users may be allowed to download data directly to their local computer from within an internet browser or with the cdis-data-client . When more security is required, users may be required to download and analyze data in a protected environment, such as a virtual machine in a virtual private cloud. Accessing data from within a browser \uf0c1 Once data files are registered , their address in s3 object storage can be obtained by providing the file's UUID to the following URL: https://data.gen3.org/index/index/UUID Data files can be downloaded by providing the file's UUID to the following URL: https://data.gen3.org/user/data/download/UUID Downloading data with the cdis-data-client \uf0c1 Data files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files. Download the latest release of the client here. Once downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal: ./cdis-data-client configure --profile profile_name --cred /path/to/api/credentials.json The client will then prompt you for the API. Enter the API of your commons, e.g.: API endpoint: https://gen3.datacommons.io/ To download a data file, pass the file's UUID to the client: cdis-data-client download --profile profile_name --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014 In the above command, download mode is specified, the profile_name we configured with the API credentails earlier is used, and a filename ( filename.tsv ) was specified for our local copy of the downloaded file. Accessing data from the Virtual Private Cloud \uf0c1 1. Send credentials and get welcome email \uf0c1 Send SSH Key and Oauth to Gen3 commons team. To access the Virtual Private Cloud (VPC), users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to . NOTE: Do not send your private ssh key. This is confidential and should never be shared with anyone. The public ssh key will be used to access the login node and any virtual machinnes (VMs) setup for the user. The email will be used to permit access to: Bionimbus to receive s3 data storage credentials data.Gen3.org for browser based querying of metadata. NOTE: for Gen3 members that were also their organization's contact for data submission, you already have access to your s3 credentials and the commons metadata API . Just send your pubkey. I'm not familiar with SSH - how do I generate a keypair? Github has a very nice ssh tutorial with step-by-step instructions for Mac, Windows (users with gitbash installed), and Linux users. If you're new to using SSH we'd recommend reviewing the links: About SSH Checking for Existing SSH Keys Generating a new SSH key and adding it to the SSH Agent NOTE: For Windows users, we recommend installing Git for Windows and using the Git Bash feature to interact on the command line, manage ssh keys, and s3 credentials. Receive a welcome email Gen3 users with the appropriate signed legal documents will be sent an email that gives the following unique information: username (this is used to ssh to the VPC login node) - eg: ssh -A username @ login-ip-address an IP associated with your new VM 2. SSH to Virtual Machine: config \uf0c1 How will I access the Login Node and my Virtual Machine? Gen3 Commons users may login to the Virtual Private Cloud (VPC) headnode, then hop over to an analysis virtual machine (VM). For more information on the VPC architecture . In your welcome email you received your username and your vm. In order to access your VM, you first must access the VPC login node. This configuration helps ensure the security of the Gen3 commons by having your VM in a private subnet. Using the credentials from your welcome email this can be done in the following order: SSH to login node: ssh -A username @3 login-node-IP SSH from login node to your VM: ssh -A ubuntu@ my_VM_IP NOTE 1: 34.197.164.18 is the IP for the login node. This is unlikely to change. NOTE 2: the -A flag forwards all the keys in your key chain. For more details on managing SSH keys, check the guides linked in the previous step . NOTE 3: You can't login to your analysis VM (in the private subnet) without first logging in to the login node (in the public subnet). Advanced users can manage these connections however they see fit. For other users, we recommend updating your SSH config file so you can setup a 'multihop' ssh tunnel. To 'multihop' in this context is to use a single command to get to your VM. What follows are instructions for updating your .ssh/config file to get to your VM in a single command. 3. Setting up an ssh config for 'multihop' \uf0c1 To start, go to your .ssh directory in your laptop home directory. cd ~/.ssh If this directory does not exist, make one. mkdir -p ~/.ssh cd ~/.ssh Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]: vim config In this file you can specify various hosts for access via ssh. Your host for the head login node should look something like this: Host login-node Hostname ip.address.of.login.node User YOUR_USERNAME IdentityFile /path/to/YOUR_CREDFILE ForwardAgent yes Where /path/to/YOUR_CREDFILE might be, e.g., ~/.ssh/id_rsa The username will be provided to you by the Gen3 support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the Gen3 head-node using this host: ssh login-node Exit the head login node and return to your local machine. Back in your config file, add a new host for your analysis VM: Host analysis Hostname YOUR_VM_IP User ubuntu ProxyCommand ssh -q -AXY login-node -W %h:%p Once again you will receive the Hostname IP from the Gen3 support team in step 4. This host will route you through the head login node and take you directly to your personal Gen3 analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM: ssh analysis If you've done everything correctly, you should now be in the analysis VM. 4. Access \"raw\" data storage from Virtual Machine \uf0c1 Add s3 'raw' data storage credentials to your VM Now you'll need to add your storage credentials to your analysis VM. Details on getting your credentials from the Bionimbus storage portal are outlined in the data contribution section of this documentation. If you are only accessing data and did not contribute, please follow those directions to acquire your keys using the Oauth you provided in Step 1 of the Data Access section. If you did contribute data you should use an existing key. They will still have \"read/write\" permission to your project folder, but will also have permission to \"read\" other data in the Gen3 commons. If you submitted multiple projects and have multiple keys, all will have the same \"read\" permissions for data - it only matters which one you pick if you still intend to write data to your project folder from your VM. As a reminder, the command to setup a profile is: aws configure --profile CREATE YOUR PROFILE NAME Example: Configure an s3 profile in your VM Review contents of the Gen3 Commons You can now review the 'raw' data folders in the Gen3 object storage. aws s3 ls s3://commons-data/ --profile profilename To peek inside a folder: aws s3 ls s3://commons-data/ foldername / --profile profilename You can use other commands to pull files from the s3 object storage into your VM. If you're not familiar with AWScli commands, we recommend reviewing the high-level docs or the complete documentation . NOTE: Remember, that since your access is read only (except any projects you've submitted associated with your keys), you will only be able to read, not write to the project folders in the commons. NOTE2: If you are using keys with write access to your project folders in the commons, be VERY CAREFUL. Don't delete any data you'll have to resubmit later. Ready to work! You're ready to use whatever tools you wish to analyze data in the commons within your VM. For requests for alternative configurations, analysis storage, or other needs please contact . For an example of how you could use a Jupyter Notebook to run analysis in the browser on your local computer, please continue on to the next section . There are lots of good examples that may be useful to you.","title":"data access June2018"},{"location":"old_verions/data-access-June2018/#accessing-data-from-within-a-browser","text":"Once data files are registered , their address in s3 object storage can be obtained by providing the file's UUID to the following URL: https://data.gen3.org/index/index/UUID Data files can be downloaded by providing the file's UUID to the following URL: https://data.gen3.org/user/data/download/UUID","title":"Accessing data from within a browser"},{"location":"old_verions/data-access-June2018/#downloading-data-with-the-cdis-data-client","text":"Data files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files. Download the latest release of the client here. Once downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal: ./cdis-data-client configure --profile profile_name --cred /path/to/api/credentials.json The client will then prompt you for the API. Enter the API of your commons, e.g.: API endpoint: https://gen3.datacommons.io/ To download a data file, pass the file's UUID to the client: cdis-data-client download --profile profile_name --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014 In the above command, download mode is specified, the profile_name we configured with the API credentails earlier is used, and a filename ( filename.tsv ) was specified for our local copy of the downloaded file.","title":"Downloading data with the cdis-data-client"},{"location":"old_verions/data-access-June2018/#accessing-data-from-the-virtual-private-cloud","text":"","title":"Accessing data from the Virtual Private Cloud"},{"location":"old_verions/data-access-June2018/#1-send-credentials-and-get-welcome-email","text":"","title":"1. Send credentials and get welcome email"},{"location":"old_verions/data-access-June2018/#2-ssh-to-virtual-machine-config","text":"","title":"2. SSH to Virtual Machine: config"},{"location":"old_verions/data-access-June2018/#3-setting-up-an-ssh-config-for-multihop","text":"To start, go to your .ssh directory in your laptop home directory. cd ~/.ssh If this directory does not exist, make one. mkdir -p ~/.ssh cd ~/.ssh Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]: vim config In this file you can specify various hosts for access via ssh. Your host for the head login node should look something like this: Host login-node Hostname ip.address.of.login.node User YOUR_USERNAME IdentityFile /path/to/YOUR_CREDFILE ForwardAgent yes Where /path/to/YOUR_CREDFILE might be, e.g., ~/.ssh/id_rsa The username will be provided to you by the Gen3 support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the Gen3 head-node using this host: ssh login-node Exit the head login node and return to your local machine. Back in your config file, add a new host for your analysis VM: Host analysis Hostname YOUR_VM_IP User ubuntu ProxyCommand ssh -q -AXY login-node -W %h:%p Once again you will receive the Hostname IP from the Gen3 support team in step 4. This host will route you through the head login node and take you directly to your personal Gen3 analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM: ssh analysis If you've done everything correctly, you should now be in the analysis VM.","title":"3. Setting up an ssh config for 'multihop'"},{"location":"old_verions/data-access-June2018/#4-access-raw-data-storage-from-virtual-machine","text":"","title":"4. Access \"raw\" data storage from Virtual Machine"},{"location":"old_verions/data-analysis-June2018/","text":"Data Analysis in the Gen3 Commons Running a Jupyter Server in the Commons Workspace \uf0c1 The Gen3 data portal provides a Workspace where users can run a Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal. Click \"Start My Server\" to start the Jupyter server in your Workspace: Or if a server is already running, click on \"My Server\" to access your files. The Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. After editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page. The following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\": This clip demonstrates creating a new Jupyter notebook in the R language: Terminal sessions can also be started in the Workspace and used to download other tools. You can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them. Running a Jupyter server on a virtual machine (VM) \uf0c1 The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information. Running the notebook in your VM \uf0c1 After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c Port forwarding to your VM \uf0c1 Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 . Access the notebook in via browser \uf0c1 In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser. Example: Run Server, port forward, access notebook in browser Review and follow the notebook \uf0c1 Credentials The notebook makes use of both a .secrets file that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the s3 profile you created to pull 'raw' data into your VM for analysis. The first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is. NOTE: If you have forgotten what you called your profile, you can always take a look at the credential file to review. From the VM run: vi ~/.aws/credentials . Jupyter Basics If you're not familiar with Jupyter notebooks, you have a few options to run the commands inside. You can review line by line (select cell - Shift+Enter ) or you can run all from the \"Kernel\" menu at the top of the browser. Shutting Down your Server When you're done working, we encourage you to shut down your Jupyter server via ctrl + c in the VM that's running it. You don't have to do this every time, but you should do it when you don't need it any more. VM Termination At this point in the Gen3 commons development, you should contact when you no longer need your VM active. Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.","title":"data analysis June2018"},{"location":"old_verions/data-analysis-June2018/#running-a-jupyter-server-in-the-commons-workspace","text":"The Gen3 data portal provides a Workspace where users can run a Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal. Click \"Start My Server\" to start the Jupyter server in your Workspace: Or if a server is already running, click on \"My Server\" to access your files. The Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. After editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page. The following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\": This clip demonstrates creating a new Jupyter notebook in the R language: Terminal sessions can also be started in the Workspace and used to download other tools. You can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them.","title":"Running a Jupyter Server in the Commons Workspace"},{"location":"old_verions/data-analysis-June2018/#running-a-jupyter-server-on-a-virtual-machine-vm","text":"The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information.","title":"Running a Jupyter server on a virtual machine (VM)"},{"location":"old_verions/data-analysis-June2018/#running-the-notebook-in-your-vm","text":"After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c","title":"Running the notebook in your VM"},{"location":"old_verions/data-analysis-June2018/#port-forwarding-to-your-vm","text":"Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 .","title":"Port forwarding to your VM"},{"location":"old_verions/data-analysis-June2018/#access-the-notebook-in-via-browser","text":"In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.","title":"Access the notebook in via browser"},{"location":"old_verions/data-analysis-June2018/#review-and-follow-the-notebook","text":"","title":"Review and follow the notebook"},{"location":"old_verions/data-contribution-June2018/","text":"Project Submission Overview \uf0c1 Sign documents, fill out forms, get credentials Prepare and submit metadata to commons API Prepare and submit full dataset (w/o metadata tsvs) to object storage. Steps to Contribute a Project to the Gen3 Commons Review and sign legal Complete the data inventory form Receive project name and access credentials Prepare metadata that fits the data model Access metadata submission portal Submit and validate project metadata tsvs Get and configure s3 data storage credentials Upload \"raw\" data to object storage * NOTE: Gen3 members are encouraged to submit multiple projects to the commons. To do so, repeat steps 2-8 above. Why Do Gen3 Commons Use a Data Model? Having all participating members use the same data model: Allows for standardized metadata elements across a commons. Permits flexible and scaleable API generation based on data commons software that reads the data model schema. Lets users query the commons API so that an ecosystem of applications can be built. Helps automate the validation of submitted data. Here is the most current data model . Here is the most current graph of the model . If you have an submission element that you believe can't be described in the model, we'd be glad to work with you to find a home for the element or update the model. Questions? Contact: For Legal: gen3-legal@occ-data.org For Submission/Access Questions: gen3-support@datacommons.io 1. Review and sign legal agreement \uf0c1 To use the Gen3 commons, please review and sign the following agreements and return them to info@gen3.org. The most current versions (and past versions for transparency) of all of the below documents can be found at: https://www.gen3.org/data-governance/ Data Contributor Agreement (DCA) Data Services/Use Agreement (DUA) If you only wish to contribute data, you do not need to sign the DUA. These documents may also reference the: Privacy and Security Agreement Intellectual Property Rights (IPR) Policy 2. Complete the data inventory form \uf0c1 Prepare a pre-submission data inventory form . Having this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form. 3. Receive project name / API credentials \uf0c1 Once you have completed the data inventory form , you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud headnode), and instructions to access the metadata submission portal and an object storage for your project. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata for your experiment nodes. Project name example mycompanyname _P0001_T1 mycompanyname _P0002_T1 mycompanyname _P0001_T2 Breakdown: \" mycompanyname>\" identifies the submitter organization \"P000x\" identifies the submission number for said organization in said train \"Tx\" identifies the train number NOTE: the Gen3 data submission calendar is broken up into different trains. NOTE2: Your project folder will have a prefix appended to it to identify the commons. eg - Gen3: Gen3_ mycompanyname _P0001_T1 4. Prepare metadata that fits the data model \uf0c1 Overview Gen3 data contributors will need to prepare metadata for their submission in tab-separated value (tsv) files, login to a portal provided for submission, and upload and validate their metadata submission. This is simultaneously the most challenging and crucial part of contributing data to a Gen3 commons. This page details the preparation and ordering of the tsvs that will be submitted. The next two pages cover submission virtual machine (VM) access and uploading/validating your metadata tsv submission. Review and understand the data model Now that you have your project name, you can begin building out the rest of your metadata. Reference the most recent graph model of your commons to help guide you on the necessary components in your submission. As you can see, from project you build up to experiment then to case and so on. For the properties that are allowed within this submission, please take some time to read through the dictionary schemas . Descriptions for each property as well as the valid submission values can be found in those schemas. Once you have access to submission portal , we recommend using the Data Dictionary Viewer to review the schema and determine which properties best describe your submission. This tool will help you understand the field types, requirements, and node dependencies for your submission. Create your TSVs It may be helpful to think of each TSV as a node in the graph of the data model. Each node can have multiple components, for example a project could have multiple experiments in a node. Blank TSV templates can be found here . Note there are wiki pages associated with each potential tsv or node in the templates. They show example fields and information about data provenance. Field types and limitations can be gleaned from a careful read of the associated yaml files . Determine Submission Order Before we discuss the actual submission process, we must first mention that the files must be submitted in a specific order. Once again referring back to the graph model , you cannot submit a node without submitting the nodes to which it points. If you submit a file out of order, the validator will reject the submission on the basis that the dependency you point to is not present (e.g. the read_groups.submitter_id in assay_result.tsv will be pointing to a node that doesn\u2019t exist). The Data Dictionary viewer can help you determine these dependencies. Sample Diagram of TSV Submission Order While this diagram represents an earlier version of the Gen3 data model, the required submission logic for current versions of the model will be very similar. Note that metadata describing data files that will be uploaded to s3 object storage need to include the file_size, md5sum, and the address of the file in s3 object storage (e.g., s3://data-bucket/folder/file.txt). Therefore, before submitting data_file metadata TSVs, make sure all of that information is included and correct. Once data_files are submitted, their metadata cannot be modified without deletion and re-creation. 5. Access metadata submission portal \uf0c1 What is the metadata submission portal? The metadata submission portal is secure environment for submitting the tsv metadata files associated with your project submission, querying the metadata associated with your project and others, and using the Data Dictionary Viewer to understand fields. You will be able to login with your OAuth and you will have access update or edit the metadata associated with your submission by adding tsv files or \"nodes\" to the graph. Where is the metadata submission portal? Links to Gen3 Commons Submission portals: Gen3 6. Submit and validate project metadata tsvs \uf0c1 Begin your submission From the submission portal select the project for which you wish to submit metadata. Remembering the order in which you need to submit your tsvs (see Determine Submission Order for details) begin your submission by uploading or copying in your first tsv (likely \"experiment.tsv\"). NOTE: If you would prefer submitting nodes in json to tsv the submission portal also accepts json. To get you started, the first node - \"project\" has already been created for you. Now you should see a lot of details about the submission process. At the very bottom, you should be able to immediately get a grasp of how the submission went with the following fields: {'entity_error_count': 0, 'message': 'Transaction successful.', 'success': True, 'transaction_id': 403, 'transactional_error_count': 0, 'transactional_errors': [], 'updated_entity_count': 40} Specifically, the message and success fields should provide you with whether your submission was valid and went through. If you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes: {'action': 'update', 'errors': [], 'id': 'asdf21as-2q4a-2563-213k-8dn4kg8dsb3j', 'related_cases': [], 'type': 'case', 'unique_keys': [{'project_id': 'gen3-MyGroup_P0001_T1' 'submitter_id': u'gen3_MG_P0001_EX1_C1'}], 'valid': True, 'warnings': []}, On a successful submission, the API will return something like above. The action can be used to identify if the node was created new or just updated. Other useful information includes the id for the node. This is the UUID for the submission and is unique for the node throughout the entirety of the Gen3 Commons. The other unique_key provided is the tuple project_id and submitter_id; another way of saying that the submitter_id combined with the project_id is a universal identifier for this node. Troubleshooting and finishing your submission If, against all odds, your submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission. The best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what tripped our validation check. The most common problems are simple issues such as spelling errors or mislabeled fields. Other errors include the one I mentioned earlier about submitting out of order and errors from not adhering to valid values as defined in our dictionary. If you don't receive any output it usually means your TSV is invalid and must be edited. Submission Failure Example ALTERNATIVE in BETA: Form submission Using tsvs allows users to bulk submit their metadata. This is ultimately a much faster process for users with larger datasets. If you wish, you can also use form submission to insert properties into a node. NOTE: if you use the same submitter ID, and submit again to the same node, it will overwrite properties. Be sure to change it should you choose to use form submission multiple times on a given node. NOTE2: it is not currently possible to remove values submitted in error using the form submission method. Query the API If helpful you can query the API in the submission portal to confirm or correct your submission, to delete nodes, or to submit json queries to learn more about your submission or all the data in the commons. To learn more visit the API section of the wiki . Check the commons matrices Read \"What's an example of the API at work?\" to learn more about how the data matrices work. Suffice to say, you can check them hourly to see relevant metadata you submit appear! Provide feedback You may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the Gen3 support team at gen3-support@occ-data.org. We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values. Let us know submission is complete Please contact the Gen3 support team to let us know when your submission is complete. Submission FAQ How do I know my submission was accepted? You will see this within the json output: 'message': 'Transaction successful.', 'success': True, How can I learn more about the elements of my existing submission? When you are viewing a project, there is a \"browse nodes\" feature. From here you can download, view, or completely delete the tsvs associated with any project you have write access to. What happens if i need to resubmit a tsv that was already accepted? When you resubmit (aka submit to a node with the same submitter id), you will update the existing node. For example, if you submit a sample with sample_type , composition , and tube_type , and you later realize that the tube type was wrong for that submission, if you were to resubmit a tsv that has just the submitter_id and tube_type it will overwrite ONLY the tube type. The sample_type and composition from the previous submission will still be in the database. What this means is that if you had previously submitted something you DON'T want, like say you realize that you accidentally submitted the tube_type as method_of_sample_procurement , simply renaming the header in your TSV would not overwrite the existing data in the node. You would need to submit null/empty values for method_of_sample_procurement to get rid of it. I was part of the very first submission group, in Nov/Dec 2016 when we added TSVs directly into an object store. Where is the `project.submitter_id` field? projects.submitter_id has become projects.code now. SO: EXAMPLE: program.name = 'gen3' project.code ='MyOrg_P0001_T1' * project_id = program.name + '-' + project.code = 'gen3-MyOrg_P0001_T1' 7. Get and configure s3 data storage credentials \uf0c1 Overview Now that you've successfully submitted and validated your project metadata, it's time to upload your 'raw' data to the Gen3 Commons. The Gen3 commons utilizes object storage . This page details how users gain and manage the credentials to access a project folder. The following page will detail submitting data to the project folder. Why should we use command line to submit data instead of some kind of website? Because for transfer of large files or many files, there can be time-out issues, encryption issues, or corruption issues. Using the command line ensures secure and complete transfer. NOTE: if you only have small files and are uncomfortable operating on the command line, you may want to try using a GUI tool like Cyberduck to connect to S3 and manage your upload instead. Obtain S3 credentials Go to https://bionimbus-pdc.opensciencedatacloud.org/storage/ Select and use the authentication method you gave in the data inventory form Download access keys by selecting the appropriate button next to the information for your username, e.g., under \"Available Gen3 Datasets\". The button is labeled \u201cGenerate S3 credential.\u201d See Figure 1. Credentials appear as comma-separated values including an access key and a secret key. The secret key should remain secret. Do not share these keys. NOTE: Do not share the s3 credentials you gain below with anyone. If you need to obtain new credentials, repeat steps (2.a) through (2.c). The button will now say \u201cRotate key\u201d button, which will deactivate previous credentials and provide new ones. KNOWN ISSUE: Safari will not provide S3 credentials on the first try. After generating keys once, press the \u201cRotate key\u201d button. Install AWS CLI Mac OSX or Linux/Unix Open your \u201cTerminal\u201d application and type the following command: sudo apt-get install awscli or sudo easy_install awscli If prompted, enter your device\u2019s password. Windows Download and run one of the windows installers at: https://aws.amazon.com/cli/ Configure AWS CLI with the downloaded project credentials In your \u201cTerminal\u201d application, type the line below, replacing \u201cCREATE YOUR PROFILE NAME\u201d with a profile name you create. aws configure --profile CREATE YOUR PROFILE NAME Pro-tip: Profile management will be very important as groups submit multiple projects. Your downloaded s3 credentials only give you access to the project folder for which they are created. If you are only submitting one project, you don't need profiles. If you are submitting multiple, you will want to carefully pick project names for each project. eg - P0001_T1 and P0002_T1. Some good sources on managing multiple profiles: OSX/Linux: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles Windows powershell: http://docs.aws.amazon.com/powershell/latest/userguide/specifying-your-aws-credentials.html After pressing Enter/Return, you will be prompted for your Access Key ID and Secret Access Key. These are the S3 credentials you downloaded earlier. Enter keys as prompted, pressing Enter/Return after each step. AWS Access Key ID [****************]: AWS Secret Access Key [****************]: Enter a default region name, as prompted - us-east-1. Press Enter/Return. Default region name [None]: us-east-1 Press return at the final prompt. Default output format [None]: You should now be able to see the folder names in the gen3-data bucket. aws s3 ls s3://gen3-data/ --profile profilename 8. Upload \"raw\" data to object storage \uf0c1 Preparing your data Data files such as BAMs, FASTQs, or PDFs should be uploaded directly to the object store. The metadata TSVs you prepared should not be submitted to the object store, as they have already been submitted via the API. Please prepare a single folder with all of submission files in the same directory. No sub directories. Uploading your data You can now upload all the files in the prepared folder on your local computer using the AWS CLI and the profiles you configured in step 7 . Below is the command to copy a folder filled with all your submission files on your local computer to your project folder in a commons. You need to change the name of /path/folder/ to the name of the path of the folder you want to upload. aws s3 cp --sse AES256 /local/folder/path/ s3://gen3-data/ remote/folder/path/ --recursive --profile profile_name EXTRA: In an object store, a \"folder\" or \"dir\" can't exist with nothing in it. Thus, if you were to 'ls' before moving any files into it, you wouldn't see the project folder you have access to. In the example above you're essentially uploading all your data and \"creating a folder\" in a single step. The following command synchronizes a local folder with a remote s3 folder. Any files already existing in the remote folder won't be copied: aws s3 sync --sse AES256 /local/path/ s3://gen3-data-bucket/ remote/path --profile profile_name Other useful commands and AWS CLI documentation can be found at: https://www.opensciencedatacloud.org/support/pdc.html and https://aws.amazon.com/cli/ 9. Register data files in storage with the Gen3 data portal \uf0c1 Once contributed data files have been uploaded to s3 object storage and the metadata describing them have been submitted to the Gen3 data portal, the files must be \"registered\" in order to be downloadable using the data portal file explorer or cdis-data-client. File registration is achieved by submitting the URL address of files in s3 as the metadata property \"urls\". For example, say the following three files listed in the s3 bucket 's3://gen3-data/folder1/' need to be registered: ~$ aws s3 ls s3://gen3-data/folder1/ --profile myprofile 2018-02-05 11:12:18 43449 file1.txt 2018-02-06 09:12:33 80416 file2.txt 2018-01-25 15:26:22 93839 file3.txt If the metadata TSV describing these files was already submitted, it can simply be updated by adding a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,: type filename file_size etc... urls data_file file1.txt 43449 ... s3://gen3-data/folder1/file1.txt data_file file2.txt 80416 ... s3://gen3-data/folder1/file2.txt data_file file3.txt 93839 ... s3://gen3-data/folder1/file3.txt Note 1: Once the data files are registered, their metadata cannot be changed. If changes need to be made at this point, the metadata record/entity must be deleted and re-created. Note 2: Once files are registered, they can be downloaded using the cdis-data-client or using the file explorer . Appendix: Data Dictionary Viewer \uf0c1 The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: https://data.Gen3.org/dd/ The Data Dictionary Viewer lets you browse and understand the available fields in a node and review the dependencies a given node has to the existence of a prior node. This is an invaluable tool for both the submission of data and later analysis of the entire commons. In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables. Appendix: Managing timepoints in a submission \uf0c1 Some elements of submitted datasets could be related to each other in time. To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates. Every other date field is anchored by the \"index_date\" in the \"case\" node. In this field you can have things like \"Study Enrollment\" or \"Diagnosis\". Study the case node in the dictionary for more information on the index_date field: https://data.Gen3.org/dd/case Examples of submissions using multiple date times Patient A enrolls in a study on July 1, and has a blood sample taken on July 10. For patient A you would report: case.index_date = \"Study Enrollment\" biospecimen.days_to_procurement = July 10 - July 1 = 9 Alternatively if they were diagnosed 2 years before the study began and you wanted to use that as the index_date nothing is stopping you: case.index_date = \"Diagnosis\" biospecimen.days_to_procurment = July 10, 2017 - July 1, 2015 = 739 Negative Dates Days to values can also be negative. If you have an index_date event that occurs after the event, you would present those days_to values as negative. If Patient A had a biospecimen taken when they were initially diagnosed: case.index_date = \"Study Enrollment\" biospecimen.days_to_procurement = July 10, 2015 - July 1, 2017 = -721 No Time Series The days_to_procurement and days_to_collection are required fields. If you do not have any data for these, we allow escape values of \"Unknown\" and \"Not Applicable\". Please use \"Unknown\" in the instances where you have established a time series but are unable to pin down the date of the event. Use \"Not Applicable\" if you do not have a time series at all.","title":"data contribution June2018"},{"location":"old_verions/data-contribution-June2018/#project-submission-overview","text":"Sign documents, fill out forms, get credentials Prepare and submit metadata to commons API Prepare and submit full dataset (w/o metadata tsvs) to object storage.","title":"Project Submission Overview"},{"location":"old_verions/data-contribution-June2018/#1-review-and-sign-legal-agreement","text":"To use the Gen3 commons, please review and sign the following agreements and return them to info@gen3.org. The most current versions (and past versions for transparency) of all of the below documents can be found at: https://www.gen3.org/data-governance/ Data Contributor Agreement (DCA) Data Services/Use Agreement (DUA) If you only wish to contribute data, you do not need to sign the DUA. These documents may also reference the: Privacy and Security Agreement Intellectual Property Rights (IPR) Policy","title":"1. Review and sign legal agreement"},{"location":"old_verions/data-contribution-June2018/#2-complete-the-data-inventory-form","text":"Prepare a pre-submission data inventory form . Having this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form.","title":"2. Complete the data inventory form"},{"location":"old_verions/data-contribution-June2018/#3-receive-project-name-api-credentials","text":"Once you have completed the data inventory form , you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud headnode), and instructions to access the metadata submission portal and an object storage for your project. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata for your experiment nodes.","title":"3. Receive project name / API credentials"},{"location":"old_verions/data-contribution-June2018/#4-prepare-metadata-that-fits-the-data-model","text":"","title":"4. Prepare metadata that fits the data model"},{"location":"old_verions/data-contribution-June2018/#5-access-metadata-submission-portal","text":"","title":"5. Access metadata submission portal"},{"location":"old_verions/data-contribution-June2018/#6-submit-and-validate-project-metadata-tsvs","text":"","title":"6. Submit and validate project metadata tsvs"},{"location":"old_verions/data-contribution-June2018/#7-get-and-configure-s3-data-storage-credentials","text":"","title":"7. Get and configure s3 data storage credentials"},{"location":"old_verions/data-contribution-June2018/#8-upload-raw-data-to-object-storage","text":"","title":"8. Upload \"raw\" data to object storage"},{"location":"old_verions/data-contribution-June2018/#9-register-data-files-in-storage-with-the-gen3-data-portal","text":"Once contributed data files have been uploaded to s3 object storage and the metadata describing them have been submitted to the Gen3 data portal, the files must be \"registered\" in order to be downloadable using the data portal file explorer or cdis-data-client. File registration is achieved by submitting the URL address of files in s3 as the metadata property \"urls\". For example, say the following three files listed in the s3 bucket 's3://gen3-data/folder1/' need to be registered: ~$ aws s3 ls s3://gen3-data/folder1/ --profile myprofile 2018-02-05 11:12:18 43449 file1.txt 2018-02-06 09:12:33 80416 file2.txt 2018-01-25 15:26:22 93839 file3.txt If the metadata TSV describing these files was already submitted, it can simply be updated by adding a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,: type filename file_size etc... urls data_file file1.txt 43449 ... s3://gen3-data/folder1/file1.txt data_file file2.txt 80416 ... s3://gen3-data/folder1/file2.txt data_file file3.txt 93839 ... s3://gen3-data/folder1/file3.txt Note 1: Once the data files are registered, their metadata cannot be changed. If changes need to be made at this point, the metadata record/entity must be deleted and re-created. Note 2: Once files are registered, they can be downloaded using the cdis-data-client or using the file explorer .","title":"9. Register data files in storage with the Gen3 data portal"},{"location":"old_verions/data-contribution-June2018/#appendix-data-dictionary-viewer","text":"The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: https://data.Gen3.org/dd/ The Data Dictionary Viewer lets you browse and understand the available fields in a node and review the dependencies a given node has to the existence of a prior node. This is an invaluable tool for both the submission of data and later analysis of the entire commons. In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.","title":"Appendix: Data Dictionary Viewer"},{"location":"old_verions/data-contribution-June2018/#appendix-managing-timepoints-in-a-submission","text":"Some elements of submitted datasets could be related to each other in time. To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates. Every other date field is anchored by the \"index_date\" in the \"case\" node. In this field you can have things like \"Study Enrollment\" or \"Diagnosis\". Study the case node in the dictionary for more information on the index_date field: https://data.Gen3.org/dd/case","title":"Appendix: Managing timepoints in a submission"},{"location":"old_verions/demo-June2018/","text":"DEMO: Using a Jupyter notebook for analysis \uf0c1 The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information. Running the notebook in your VM \uf0c1 After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c Port forwarding to your VM \uf0c1 Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 . Access the notebook in via browser \uf0c1 In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser. Example: Run Server, port forward, access notebook in browser Review and follow the notebook \uf0c1 Credentials The notebook makes use of both a .secrets file that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the s3 profile you created to pull 'raw' data into your VM for analysis. The first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is. NOTE: If you have forgotten what you called your profile, you can always take a look at the credential file to review. From the VM run: vi ~/.aws/credentials . Jupyter Basics If you're not familiar with Jupyter notebooks, you have a few options to run the commands inside. You can review line by line (select cell - Shift+Enter ) or you can run all from the \"Kernel\" menu at the top of the browser. Shutting Down your Server When you're done working, we encourage you to shut down your Jupyter server via ctrl + c in the VM that's running it. You don't have to do this every time, but you should do it when you don't need it any more. VM Termination At this point in the Gen3 commons development, you should contact when you no longer need your VM active. Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.","title":"DEMO: Using a Jupyter notebook for analysis"},{"location":"old_verions/demo-June2018/#demo-using-a-jupyter-notebook-for-analysis","text":"The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information.","title":"DEMO: Using a Jupyter notebook for analysis"},{"location":"old_verions/demo-June2018/#running-the-notebook-in-your-vm","text":"After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c","title":"Running the notebook in your VM"},{"location":"old_verions/demo-June2018/#port-forwarding-to-your-vm","text":"Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 .","title":"Port forwarding to your VM"},{"location":"old_verions/demo-June2018/#access-the-notebook-in-via-browser","text":"In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.","title":"Access the notebook in via browser"},{"location":"old_verions/demo-June2018/#review-and-follow-the-notebook","text":"","title":"Review and follow the notebook"},{"location":"old_verions/guide-overview-June2018/","text":"User Guide Overview \uf0c1 This documentation includes the following user guides for assisting contributors with submitting their data to the Gen3 data commons and accessing their data: How to submit your data to the Gen3 data commons How to query and access data in the Gen3 commons How to analyze data in the Jupyter Workspace Appendices \uf0c1 Working with the API Working with the Proxy and Whitelist Data Dictionary Viewer Template Metadata TSVs Managing Submission Timepoints","title":"guide overview June2018"},{"location":"old_verions/guide-overview-June2018/#user-guide-overview","text":"This documentation includes the following user guides for assisting contributors with submitting their data to the Gen3 data commons and accessing their data: How to submit your data to the Gen3 data commons How to query and access data in the Gen3 commons How to analyze data in the Jupyter Workspace","title":"User Guide Overview"},{"location":"old_verions/guide-overview-June2018/#appendices","text":"Working with the API Working with the Proxy and Whitelist Data Dictionary Viewer Template Metadata TSVs Managing Submission Timepoints","title":"Appendices"},{"location":"user-guide/data-access/","text":"Data Access Overview The sponsor of a Gen3 data commons typically decides how users will access data in object storage. In some cases, approved users may be allowed to download data directly to their local computer from within an internet browser or with the cdis-data-client . When more security is required, users may be required to download and analyze data in a protected environment, such as a virtual machine (VM) in a virtual private cloud (VPC). Accessing data from within a browser \uf0c1 Once data files are registered , their address in s3 object storage can be obtained by providing the file's UUID to the following URL: https://data.gen3.org/index/index/UUID Data files can be downloaded by providing the file's UUID to the following URL: https://data.gen3.org/user/data/download/UUID Downloading data with the cdis-data-client \uf0c1 Data files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files. Download the latest release of the client here. Once downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal: ./cdis-data-client configure --profile profile_name --cred /path/to/api/credentials.json The client will then prompt you for the API. Enter the API of your commons, e.g.: API endpoint: https://gen3.datacommons.io/ To download a data file, pass the file's UUID to the client: cdis-data-client download --profile profile_name --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014 In the above command, download mode is specified, the profile_name we configured with the API credentails earlier is used, and a filename ( filename.tsv ) was specified for our local copy of the downloaded file. Accessing data from the Virtual Private Cloud \uf0c1 If additional security is required, the cdis-data-client can be installed on a VM in the VPC to download files. The following instructions detail how to login to a VM. Send public SSH Key and OpenConnectID account to Gen3 commons team. To access the Virtual Private Cloud (VPC), users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to . Do not send your private ssh key! This is confidential and should never be shared with anyone. Github has a very nice ssh tutorial with step-by-step instructions for Mac, Windows (users with gitbash installed), and Linux users. If you're new to using SSH we'd recommend reviewing the links: About SSH Checking for Existing SSH Keys Generating a new SSH key and adding it to the SSH Agent Gen3 users with the appropriate signed legal documents will be sent an email that gives the following unique information: username (this is used to ssh to the VPC login node) - eg: ssh -A username @ login-ip-address an IP associated with your new VM SSH to Virtual Machine In some data commons, you may need to ssh in to a \"login\" node prior to accessing your personal virtual machine: SSH to login node: ssh -A username @3 login-node-IP SSH from login node to your VM: ssh -A ubuntu@ my_VM_IP NOTE: the -A flag forwards all the keys in your key chain. For more details on managing SSH keys, check the guides linked in the previous step . It is recommended to update the SSH config file to use a 'multihop' ssh tunnel. To 'multihop' in this context is to use a single command to get to your VM. What follows are instructions for updating your .ssh/config file to get to your VM in a single command. Go to your .ssh directory in your laptop home directory: cd ~/.ssh Within this directory, edit the file named \"config\" (if it doesn't exist, create it first): vim config In this file you can specify various hosts for access via ssh. Your host for the head login node should look something like this: Host login-node Hostname ip.address.of.login.node User YOUR_USERNAME IdentityFile /path/to/YOUR_CREDFILE ForwardAgent yes Where /path/to/YOUR_CREDFILE might be: ~/.ssh/id_rsa The username will be provided to you by the Gen3 support team and will be tied to the credential file that you provide us when setting up the account (your private ssh key). Save this file and exit. Back in the command line terminal you should now be able to log in to the login node using this host: ssh login-node Exit the head login node and return to your local machine. Back in your config file, add a new host for your analysis VM: Host analysis Hostname YOUR_VM_IP User ubuntu ProxyCommand ssh -q -AXY login-node -W %h:%p Once again you will receive the Hostname IP from the support team. This host will route you through the head login node and take you directly to your personal analysis VM. Once again, save the file and exit. In the terminal, try to log in to the submission VM: ssh analysis If you've done everything correctly, you should now be in the analysis VM. Ready to work! You're ready to use whatever tools you wish to analyze data in the commons within your VM. For requests for alternative configurations, analysis storage, or other needs please contact support@datacommons.io . For an example of how to use a Jupyter Notebook for data analyses in a browser window on your local computer, please continue on to the next section .","title":"Data Access"},{"location":"user-guide/data-access/#accessing-data-from-within-a-browser","text":"Once data files are registered , their address in s3 object storage can be obtained by providing the file's UUID to the following URL: https://data.gen3.org/index/index/UUID Data files can be downloaded by providing the file's UUID to the following URL: https://data.gen3.org/user/data/download/UUID","title":"Accessing data from within a browser"},{"location":"user-guide/data-access/#downloading-data-with-the-cdis-data-client","text":"Data files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files. Download the latest release of the client here. Once downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal: ./cdis-data-client configure --profile profile_name --cred /path/to/api/credentials.json The client will then prompt you for the API. Enter the API of your commons, e.g.: API endpoint: https://gen3.datacommons.io/ To download a data file, pass the file's UUID to the client: cdis-data-client download --profile profile_name --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014 In the above command, download mode is specified, the profile_name we configured with the API credentails earlier is used, and a filename ( filename.tsv ) was specified for our local copy of the downloaded file.","title":"Downloading data with the cdis-data-client"},{"location":"user-guide/data-access/#accessing-data-from-the-virtual-private-cloud","text":"If additional security is required, the cdis-data-client can be installed on a VM in the VPC to download files. The following instructions detail how to login to a VM.","title":"Accessing data from the Virtual Private Cloud"},{"location":"user-guide/data-analysis/","text":"Data Analysis in the Gen3 Commons How data is accessed in a Gen3 Data Commons must be agreed upon by the sponsor(s), data contributor(s), and the operator(s). Some data commons have rules that data cannot be downloaded outside of a Virtual Private Cloud. In these cases, data analysts may need to access and configure a virtual machine (VM) in the VPC where all analyses will be done. Other data commons may be able to grant users permissions to download data files directly to their local computers. JupyterHub: Using Windmill's Workspace \uf0c1 The Windmill data portal may provide a Workspace where users can access a personalized Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal. Click \"Start My Server\" to start the Jupyter server in your Workspace: Or if a server is already running, click on \"My Server\" to access your files. The Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. After editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page. The following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\": This clip demonstrates creating a new Jupyter notebook in the R language: Terminal sessions can also be started in the Workspace and used to download other tools. You can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them. Running a Jupyter server on a virtual machine (VM) \uf0c1 The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic data commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results Running the notebook in your VM \uf0c1 After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c Port forwarding to your VM \uf0c1 Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 . Access the notebook in via browser \uf0c1 In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser. Example: Run Server, port forward, access notebook in browser Review and follow the notebook \uf0c1 Credentials The notebook makes use of both a .secrets file that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the s3 profile you created to pull 'raw' data into your VM for analysis. The first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is. NOTE: If you have forgotten what you called your profile, you can always take a look at the credential file to review. From the VM run: vi ~/.aws/credentials . Jupyter Basics If you're not familiar with Jupyter notebooks, you have a few options to run the commands inside. You can review line by line (select cell - Shift+Enter ) or you can run all from the \"Kernel\" menu at the top of the browser. Shutting Down your Server When you're done working, we encourage you to shut down your Jupyter server via ctrl + c in the VM that's running it. You don't have to do this every time, but you should do it when you don't need it any more. VM Termination At this point in the Gen3 commons development, you should contact support@datacommons.io when you no longer need your VM active. Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.","title":"Data Analysis"},{"location":"user-guide/data-analysis/#jupyterhub-using-windmills-workspace","text":"The Windmill data portal may provide a Workspace where users can access a personalized Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal. Click \"Start My Server\" to start the Jupyter server in your Workspace: Or if a server is already running, click on \"My Server\" to access your files. The Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. After editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page. The following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\": This clip demonstrates creating a new Jupyter notebook in the R language: Terminal sessions can also be started in the Workspace and used to download other tools. You can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them.","title":"JupyterHub: Using Windmill's Workspace"},{"location":"user-guide/data-analysis/#running-a-jupyter-server-on-a-virtual-machine-vm","text":"The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic data commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results","title":"Running a Jupyter server on a virtual machine (VM)"},{"location":"user-guide/data-analysis/#running-the-notebook-in-your-vm","text":"After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c","title":"Running the notebook in your VM"},{"location":"user-guide/data-analysis/#port-forwarding-to-your-vm","text":"Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 .","title":"Port forwarding to your VM"},{"location":"user-guide/data-analysis/#access-the-notebook-in-via-browser","text":"In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.","title":"Access the notebook in via browser"},{"location":"user-guide/data-analysis/#review-and-follow-the-notebook","text":"","title":"Review and follow the notebook"},{"location":"user-guide/data-contribution/","text":"Project Submission Overview \uf0c1 Sign relevant legal documents, fill out forms, and get credentials. Prepare and submit project metadata to the Windmill Data Portal. Prepare and submit data files to object storage. Steps to Contribute a Data Project to the Gen3 Commons Review and sign legal agreements Provide login account and project name Review the data model Prepare metadata TSVs for each node in your project Register data files with the Windmill data portal Submit TSVs and validate metadata Upload data files to object storage 1. Review and sign legal agreements \uf0c1 In order to ensure the rights and interests of data contributors and study participants are protected, there are legal agreements that must be signed prior to data submission, data access, or data portal access. The data commons sponsor should confirm in writing with the authorizing party (usually the data commons operator) that access requests are valid. The Gen3 data commons sponsor in collaboration with the commons operator should distribute the relevant documents to new users. Please review the referenced policies and sign and return the relevant legal agreements to \"support@datacommons.io\". CDIS recommends at least the following legal agreements be signed by and policies be available to the appropriate parties: Legal Agreements Data Contributor Agreement - signed by persons submitting data Data Use/Access Agreement - signed by Principal Investigator (PI) requesting access for organization Data Commons Services Agreement - signed by individuals using Windmill data portal services or downloading/analyzing data Policies Privacy and Security Agreement Intellectual Property Rights (IPR) Policy Publication Policy Notes: If your organization will be both contributing and analyzing data, all three docs are required. If you only wish to contribute data, you do not need to sign the DUA. Windmill accounts are not to be shared by individuals. Each individual requesting data portal access needs to sign and return a DCSA, and the PI needs to also return a DUA for the organization. It is the user's responsibility to ensure that the relevant policies are reviewed before signing legal agreements. 2. Provide Login Account and Project Name \uf0c1 Once legal documents are signed, the data commons operator will grant the individual data contributor appropriate permissions to login to and submit data to the Windmill data portal. Different permissions can be assigned at both the program and project level, and these permissions are associated with an individual's email address, which is used to login to the Windmill data portal. Step 1: The Windmill data portal supports user authentication via OpenID Connect (OIDC; e.g., a gmail account) and/or NIH login (e.g., eRA commons). Please, send the account you wish to use for login to support@datacommons.io. Step 2: Data contributors will also need to select an appropriate name for their project in the data portal. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata for a node in your project. Project name examples mycompanyname _P001 mycompanyname _P002 mycompanyname _ProjectID Breakdown: \"mycompanyname\" identifies the contributing organization \"P00x\" identifies the submission number for said organization \"ProjectID\" could also be used for a more descriptive identifier NOTE: Your project will have a prefix appended to it to identify the program. The \"program\" node exists in order to more finely tune permissions and, like the project node, is created by the commons operator. Thus, in the data portal, your project will be listed as Program-ProjectName . 3. Review the data model \uf0c1 What is the data model? Every Gen3 data commons employs a data model, which serves to describe and harmonize data sets, or organize data submitted by different contributors in a similar manner. Data harmonization facilitates cross-project analyses and is thus one of the pillars of the data commons paradigm. The data model organizes experimental metadata variables, or \"properties\", into linked categories, or \"nodes\", through the use of a data dictionary. The data dictionary lists and describes all nodes in the data model, and it also defines and describes the properties (metadata variables) in each node. For example, clinical variables like a primary cancer diagnosis or a subject's gender or race might go into the \"diagnosis\" or \"demographic\" nodes, respectively, while sample-related variables like how a tumor sample was collected and what analyte was extracted from it might go into the \"biospecimen\" or \"analyte\" nodes, respectively. Data files also have associated metadata variables like file size, format, and the file's location in object storage, and these properties are grouped into nodes that describe various types of data files, for example, \"mri_image\" for an MRI image data file. Finally, each node in the data dictionary is linked in a logical manner to other nodes, which facilitates generating a visual overview, or graphical model, of a project. The following image displays the data dictionary viewer, the 'biospecimen' node entry in the dictionary, and an example graphical model of a project in the BRAINCommons : Why Do Gen3 Commons Use a Data Model? Having all participating members use the same data model: Allows for standardized metadata elements across a commons. Permits flexible and scaleable API generation based on data commons software that reads the data model schema. Lets users query the commons API so that an ecosystem of applications can be built. Helps automate the validation of submitted data. Once you have access to the Windmill data submission portal, we recommend reviewing the commons' specific data dictionary by clicking \"Dictionary\" in the top navigation bar. Here you can determine which properties best describe your submission. This tool will help you understand the variable types, requirements, and node dependencies or links for your submission. If you have an submission element that you believe isn't currently described in the model, notify the commons support team (support@datacommons.io) with a description of the data elements that you'd like to add, and they will make sure the sponsor or data modeling working group reviews your request and finds an appropriate home for your data elements. 4. Prepare metadata TSVs for each node in your project \uf0c1 Data contributors will need to prepare metadata for their submission in tab-separated value (TSV) files for each node in their project. It may be helpful to think of each TSV as a node in the graph of the data model. Column headers in the TSV are the properties (or metadata variables) stored in that node. Each row is a \"record\" or \"entity\" in that node. Each record in every node will have a \"submitter_id\", which is a unique alphanumeric identifier for that record and is specified by the data submitter, and a \"type\", which is simply the node name. Besides the \"submitter_id\" and \"type\", which are required for every record, other properties are either required or not, and this can be determined in the data dictionary's \"Required\" column for a specific node. Example, blank TSV templates can be found here and actual template TSVs for your commons are provided in each node's page in the data dictionary. Determine Submission Order via Node Links The prepared TSV files must be submitted in a specific order due to node links. Referring back to the graphical data model, you cannot submit a node without first submitting the nodes to which it is linked upstream. If you submit a metadata record out of order, that is, if you submit a record with a link to an upstream node that doesn't yet exist, the validator will reject the submission on the basis that the dependency you point to is not present with the error message \"INVALID_LINK\". The \"program\" and \"project\" nodes are the most upstream nodes and are created by a commons administrator. So, the first node submitted by data contributor is usually the \"study\" or \"experiment\" node, which points directly upstream to the \"project\" node. Next, the study participants are recorded in the \"case\" node, and subsequently any clinical information (demographics, diagnoses, etc.), biospecimen data (biopsy samples, extracted analytes), etc., is linked to each case. Finally, metadata describing the actual raw data files to be uploaded to object storage are the last nodes submitted. Specifying Required Links At least one link is required for every record in a TSV, and sometimes multiple links should be specified. The links are specified in a TSV with the variable header \" s.submitter_id\", where is the upstream node a record is linking to. The value of this variable is the specific submitter_id of the link (the record in that upstream node which the current record is linked to). For example, if there are two studies in a project, \"study-01\" and \"study-02\", the \"case.tsv\" TSV file will be uploaded to describe the study participants enrolled in each study. Each row in the \"case.tsv\" file would describe a single study participant, and the first case has the submitter_id \"case-01\". There would be at least one link in that TSV specified with the column header \"studies.submitter_id\", and each row would have either \"study-01\" or \"study-02\" as the value for this column. Specifying Multiple Links Links can be one-to-one, many-to-one, one-to-many, and many-to-many. Since a single study participant can be enrolled in multiple studies, and a single study will have multiple cases enrolled in it, this link is \"many-to-many\". On the other hand, since a single study cannot be linked to multiple projects, but a single project can have many studies linked to it, the study - project link is \"many-to-one\". In the above example, if \"case-01\" was enrolled in both \"study-01\" and \"study-02\", then there would be two columns to specify these links in the case.tsv file: \"studies.submitter_id#1\" and \"studies.submitter_id#2\". The values would be \"study-01\" for one of them and \"study-02\" for the other. Once the \"case.tsv\" file is uploaded and creates the record \"case-01\" in the \"case\" node, if \"case-01\" had a diagnosis record linked to it, then in the \"diagnosis.tsv\" file to be uploaded next, there would be a column header \"cases.submitter_id\" and the value would be \"case-01\" (the case's \"submitter_id\") to link this diagnosis record to that case. 5. Register data files with the Windmill data portal \uf0c1 Special attention must be given to \"data file\" nodes, which house variables that describe actual, raw data files that are to up be uploaded to object storage by the data contributor and later downloaded by data analysts. Specifically, data files must be \"registered\" in order to be downloadable using the Windmill data portal or the cdis-data-client . Registration of data files simply means adding a column in the data file node's TSV named \"urls\" and entering the URL/address of each file in object storage (row in the TSV) in that column. This is usually a location in a project folder of a data commons bucket in s3 object storage, e.g.: \"s3://commons-bucket/project-name/filename\". For example, say the following local files need to be registered and then uploaded: commandline-prompt$ ls -l -rw-r--r--@ 1 username staff 6B May 30 15:18 file-1.dcm -rw-r--r--@ 1 username staff 7B May 30 15:18 file-2.dcm -rw-r--r--@ 1 username staff 8B May 30 15:18 file-3.dcm Add a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,: type submitter_id filename file_size etc... urls mri_image file-id-1 file-1.dcm 6 ... s3://commons-bucket/project-name/file1.txt mri_image file-id-2 file-2.dcm 7 ... s3://commons-bucket/project-name/file2.txt mri_image file-id-3 file-3.dcm 8 ... s3://commons-bucket/project-name/file3.txt Please make sure you check with the commons operator to make sure you have the correct commons bucket name prior to submitting a data file node TSV. Once the data files are registered, their metadata cannot be easily changed: the metadata record must be deleted and re-created. Also be aware that metadata describing data files that will be uploaded to s3 object storage need to include the file size and md5sum in addition to the address of the file in s3 object storage. Therefore, before submitting data file metadata TSVs, make sure all of that information is included and correct so that data downloaders can confirm completeness of their download via the md5sum and file size. 6. Submit TSVs and validate metadata \uf0c1 Begin your metadata TSV submissions To get you started submitting metadata TSVs, the first node, \"project\", has already been created for you by a commons administrator. Now, remembering that TSVs must be submitted for each node in a specific order, begin with the first node downstream of project, often \"study\" or \"experiment\" and continue to submit TSVs until all data file nodes are submitted and properly registered. From the Windmill data portal, click on \"Data Submission\" and then click \"Submit Data\" beside the project for which you wish to submit metadata TSVS. To submit a TSV: 1) Login to the Windmill data portal for your commons 2) Click on \"Data Submission\" in the top navigation bar 3) Click on \"Submit Data\" by the project for which you wish to submit metadata 4) Click on \"Upload File\" 5) Navigate to your TSV and click \"open\", the contents of the TSV should appear in the grey box below 6) Click \"Submit\" Now you should see a message that indicates either success (green \"succeeded: 200\") or failure (grey \"failed: 400\"). Further details can be reviewed by clicking on \"DETAILS\", which displays the API response in JSON form. Each record/entity that was submitted (each row in the TSV) gets a true/false value for \"valid\" and lists \"errors\" if it was not valid. If you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes: { action : update/create , errors : [ { keys : [ species (the property name) ], message : 'Homo sapien' is not one of ['Drosophila melanogaster', 'Homo sapiens', 'Mus musculus', 'Mustela putorius furo', 'Rattus rattus', 'Sus scrofa'] , type : ERROR } ], id : 1d4e9bb0-515d-4158-b14b-770ab5077d8b (the UUID created for this record) , related_cases : [], type : case (the node name) , unique_keys : [ { project_id : training (the project name) , submitter_id : training-case-02 (the record/entity submitter_id) } ], valid : false, warnings : [] } The \"action\" above can be used to identify if the node was created new or just updated; when you resubmit, that is, submit to a node with the same submitter id, you will update the existing node. Other useful information includes the \"id\" for the record. This is the UUID for the record and is unique throughout the entirety of the data commons. The other \"unique_key\" provided is the tuple \"project_id\" and \"submitter_id\", which is to say the \"submitter_id\" combined with the \"project_id\" is a universal identifier for this record. To confirm that a data file is properly registered, enter the UUID of a data file record in the index API endpoint of your data commons: usually \"data.commons-name.org/index/index/UUID\", where \"data.commons-name.org\" is the URL of the Windmill data portal and UUID is the specific UUID of a registered data file. You should see a JSON response that contains the url that was registered. If the record was not registered successfully, you will likely see an error message (you must be logged in or you will get an \"access denied\" type error). Troubleshooting and finishing your submission If, against all odds, your TSV submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission. The best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what failed the validation check. The most common problems are simple issues such as spelling errors, mislabeled properties, or missing required fields. Provide feedback Please contact the support team to let us know when your submission is complete. You may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the commons support team at support@datacommons.io . We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values. How can I learn more about my existing submission? When you are viewing a project, you can click on a node name to view the records in that node. From here you can download, view, or completely delete records associated with any project you have delete access to. 7. Upload data files to object storage \uf0c1 Preparing your data Data files such as sequencing data (BAM, FASTQ), assay results, images, PDFs, etc., should be uploaded with the CDIS data client. For detailed instructions, visit the cdis-data client documentation . The metadata TSVs you prepared do not need to be submitted to the object store, as they have already been submitted via the API. Downloaded the compiled binary for your operating system. Configure a profile with credentials: ./cdis-data-client configure --profile profile --cred credentials.json Upload a data file using its UUID: ./cdis-data-client upload --profile profile --uuid UUID --file= filename","title":"Data Contribution"},{"location":"user-guide/data-contribution/#project-submission-overview","text":"Sign relevant legal documents, fill out forms, and get credentials. Prepare and submit project metadata to the Windmill Data Portal. Prepare and submit data files to object storage.","title":"Project Submission Overview"},{"location":"user-guide/data-contribution/#1-review-and-sign-legal-agreements","text":"In order to ensure the rights and interests of data contributors and study participants are protected, there are legal agreements that must be signed prior to data submission, data access, or data portal access. The data commons sponsor should confirm in writing with the authorizing party (usually the data commons operator) that access requests are valid. The Gen3 data commons sponsor in collaboration with the commons operator should distribute the relevant documents to new users. Please review the referenced policies and sign and return the relevant legal agreements to \"support@datacommons.io\". CDIS recommends at least the following legal agreements be signed by and policies be available to the appropriate parties: Legal Agreements Data Contributor Agreement - signed by persons submitting data Data Use/Access Agreement - signed by Principal Investigator (PI) requesting access for organization Data Commons Services Agreement - signed by individuals using Windmill data portal services or downloading/analyzing data Policies Privacy and Security Agreement Intellectual Property Rights (IPR) Policy Publication Policy Notes: If your organization will be both contributing and analyzing data, all three docs are required. If you only wish to contribute data, you do not need to sign the DUA. Windmill accounts are not to be shared by individuals. Each individual requesting data portal access needs to sign and return a DCSA, and the PI needs to also return a DUA for the organization. It is the user's responsibility to ensure that the relevant policies are reviewed before signing legal agreements.","title":"1. Review and sign legal agreements"},{"location":"user-guide/data-contribution/#2-provide-login-account-and-project-name","text":"Once legal documents are signed, the data commons operator will grant the individual data contributor appropriate permissions to login to and submit data to the Windmill data portal. Different permissions can be assigned at both the program and project level, and these permissions are associated with an individual's email address, which is used to login to the Windmill data portal. Step 1: The Windmill data portal supports user authentication via OpenID Connect (OIDC; e.g., a gmail account) and/or NIH login (e.g., eRA commons). Please, send the account you wish to use for login to support@datacommons.io. Step 2: Data contributors will also need to select an appropriate name for their project in the data portal. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you submit the metadata for a node in your project.","title":"2. Provide Login Account and Project Name"},{"location":"user-guide/data-contribution/#3-review-the-data-model","text":"","title":"3. Review the data model"},{"location":"user-guide/data-contribution/#4-prepare-metadata-tsvs-for-each-node-in-your-project","text":"Data contributors will need to prepare metadata for their submission in tab-separated value (TSV) files for each node in their project. It may be helpful to think of each TSV as a node in the graph of the data model. Column headers in the TSV are the properties (or metadata variables) stored in that node. Each row is a \"record\" or \"entity\" in that node. Each record in every node will have a \"submitter_id\", which is a unique alphanumeric identifier for that record and is specified by the data submitter, and a \"type\", which is simply the node name. Besides the \"submitter_id\" and \"type\", which are required for every record, other properties are either required or not, and this can be determined in the data dictionary's \"Required\" column for a specific node. Example, blank TSV templates can be found here and actual template TSVs for your commons are provided in each node's page in the data dictionary.","title":"4. Prepare metadata TSVs for each node in your project"},{"location":"user-guide/data-contribution/#5-register-data-files-with-the-windmill-data-portal","text":"Special attention must be given to \"data file\" nodes, which house variables that describe actual, raw data files that are to up be uploaded to object storage by the data contributor and later downloaded by data analysts. Specifically, data files must be \"registered\" in order to be downloadable using the Windmill data portal or the cdis-data-client . Registration of data files simply means adding a column in the data file node's TSV named \"urls\" and entering the URL/address of each file in object storage (row in the TSV) in that column. This is usually a location in a project folder of a data commons bucket in s3 object storage, e.g.: \"s3://commons-bucket/project-name/filename\". For example, say the following local files need to be registered and then uploaded: commandline-prompt$ ls -l -rw-r--r--@ 1 username staff 6B May 30 15:18 file-1.dcm -rw-r--r--@ 1 username staff 7B May 30 15:18 file-2.dcm -rw-r--r--@ 1 username staff 8B May 30 15:18 file-3.dcm Add a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,: type submitter_id filename file_size etc... urls mri_image file-id-1 file-1.dcm 6 ... s3://commons-bucket/project-name/file1.txt mri_image file-id-2 file-2.dcm 7 ... s3://commons-bucket/project-name/file2.txt mri_image file-id-3 file-3.dcm 8 ... s3://commons-bucket/project-name/file3.txt Please make sure you check with the commons operator to make sure you have the correct commons bucket name prior to submitting a data file node TSV. Once the data files are registered, their metadata cannot be easily changed: the metadata record must be deleted and re-created. Also be aware that metadata describing data files that will be uploaded to s3 object storage need to include the file size and md5sum in addition to the address of the file in s3 object storage. Therefore, before submitting data file metadata TSVs, make sure all of that information is included and correct so that data downloaders can confirm completeness of their download via the md5sum and file size.","title":"5. Register data files with the Windmill data portal"},{"location":"user-guide/data-contribution/#6-submit-tsvs-and-validate-metadata","text":"","title":"6. Submit TSVs and validate metadata"},{"location":"user-guide/data-contribution/#7-upload-data-files-to-object-storage","text":"","title":"7. Upload data files to object storage"},{"location":"user-guide/demo/","text":"DEMO: Using a Jupyter notebook for analysis \uf0c1 The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information. Running the notebook in your VM \uf0c1 After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c Port forwarding to your VM \uf0c1 Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 . Access the notebook in via browser \uf0c1 In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser. Example: Run Server, port forward, access notebook in browser Review and follow the notebook \uf0c1 Credentials The notebook makes use of both a .secrets file that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the s3 profile you created to pull 'raw' data into your VM for analysis. The first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is. NOTE: If you have forgotten what you called your profile, you can always take a look at the credential file to review. From the VM run: vi ~/.aws/credentials . Jupyter Basics If you're not familiar with Jupyter notebooks, you have a few options to run the commands inside. You can review line by line (select cell - Shift+Enter ) or you can run all from the \"Kernel\" menu at the top of the browser. Shutting Down your Server When you're done working, we encourage you to shut down your Jupyter server via ctrl + c in the VM that's running it. You don't have to do this every time, but you should do it when you don't need it any more. VM Termination At this point in the Gen3 commons development, you should contact when you no longer need your VM active. Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.","title":"DEMO: Using a Jupyter notebook for analysis"},{"location":"user-guide/demo/#demo-using-a-jupyter-notebook-for-analysis","text":"The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses. Both can be found in your VM in the analysis folder. They can also be found at: https://github.com/occ-data/gen3-functions . The Gen3 community is encouraged to add to the functions library or improve the notebook. NOTE: As the Gen3 community updates repositories, you can keep them up to date using git pull origin master in the functions folder. It has already been initialized to sync with this repository. NOTE2: If you receive an error when trying to do git pull , you may need to set proxies and/or either save or drop any changes you've made: # set proxies: export http_proxy= http://cloud-proxy.internal.io:3128 export https_proxy= http://cloud-proxy.internal.io:3128 # to drop changes: git stash save --keep-index git stash drop # or save changes git commit . # Update CDIS utils python libraries: git clone https://github.com/uc-cdis/cdis-python-utils.git cd cdis-python-utils sudo -E python setup.py install # unset proxies to get juypter notebook to work again unset http_proxy; unset https_proxy; What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser. In the notebook, you'll learn about basic Gen3 commons operations like: Querying the API for metadata associated with submissions Pulling data into your VM for analysis Running a simple analysis over a file and collection of files Plotting the results In the Jupyter notebook, many of the calls rely on more complex functions described in the analysis_functions file . It is worth taking the time to understand how this file works so you can use and customize it or build your own tools. We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile. Just contact for more information.","title":"DEMO: Using a Jupyter notebook for analysis"},{"location":"user-guide/demo/#running-the-notebook-in-your-vm","text":"After we're logged in to our analysis VM and in the functions directory (from home: cd functions ), run the jupyter notebook server. Run the notebook server: jupyter notebook --no-browser --port=8889 NOTE: You can stop a Juptyer server at anytime via ctrl + c","title":"Running the notebook in your VM"},{"location":"user-guide/demo/#port-forwarding-to-your-vm","text":"Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine. On a terminal session from your local machine (not in the VM) setup the connection: ssh -N -L localhost:8888:localhost:8889 analysis NOTE: In the example above \"analysis\" is the name of the ssh shortcut we setup back in step 2 .","title":"Port forwarding to your VM"},{"location":"user-guide/demo/#access-the-notebook-in-via-browser","text":"In your preferred browser and enter http://localhost:8888/; Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.","title":"Access the notebook in via browser"},{"location":"user-guide/demo/#review-and-follow-the-notebook","text":"","title":"Review and follow the notebook"},{"location":"user-guide/guide-overview/","text":"User Guide Overview \uf0c1 This documentation includes the following user guides for assisting data contributors with submitting data to a Gen3 data commons and assisting data analysts with accessing and analyzing data: How to contribute a data project to a Gen3 Data Commons How to access data in a Gen3 Data Commons How to analyze data in the Jupyter Workspace Appendices \uf0c1 Using the CDIS data client Working with the API Working with the Proxy and Whitelist Data Dictionary Viewer Template Metadata TSVs Managing Submission Timepoints","title":"Guide Overview"},{"location":"user-guide/guide-overview/#user-guide-overview","text":"This documentation includes the following user guides for assisting data contributors with submitting data to a Gen3 data commons and assisting data analysts with accessing and analyzing data: How to contribute a data project to a Gen3 Data Commons How to access data in a Gen3 Data Commons How to analyze data in the Jupyter Workspace","title":"User Guide Overview"},{"location":"user-guide/guide-overview/#appendices","text":"Using the CDIS data client Working with the API Working with the Proxy and Whitelist Data Dictionary Viewer Template Metadata TSVs Managing Submission Timepoints","title":"Appendices"}]}